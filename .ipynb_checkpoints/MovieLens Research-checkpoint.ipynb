{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation \n",
    "\n",
    "\n",
    "## The MovieLens Dataset\n",
    "Contains 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6040, 5) 6040\n",
      "(3883, 3) 3883\n",
      "(1000209, 4) 1000209\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading ratings file\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating','user_emb_id'])\n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "print(users.shape, len(users))\n",
    "print(movies.shape, len(movies))\n",
    "print(ratings.shape, len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id  rating  user_emb_id\n",
      "0        1      1193       5            0\n",
      "1        1       661       3            0\n",
      "2        1       914       3            0\n",
      "3        1      3408       4            0\n",
      "4        1      2355       5            0\n"
     ]
    }
   ],
   "source": [
    "# Check the top 5 rows\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000209 entries, 0 to 1000208\n",
      "Data columns (total 4 columns):\n",
      "user_id        1000209 non-null int64\n",
      "movie_id       1000209 non-null int64\n",
      "rating         1000209 non-null int64\n",
      "user_emb_id    1000209 non-null int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 30.5 MB\n",
      "None\n",
      "Describe Ratings: \n",
      " count    1.000209e+06\n",
      "mean     3.581564e+00\n",
      "std      1.117102e+00\n",
      "min      1.000000e+00\n",
      "25%      3.000000e+00\n",
      "50%      4.000000e+00\n",
      "75%      4.000000e+00\n",
      "max      5.000000e+00\n",
      "Name: rating, dtype: float64\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f0c41a84d090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Import seaborn library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'whitegrid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Check the file info\n",
    "print(ratings.info())\n",
    "# Get summary statistics of rating\n",
    "print(\"Describe Ratings: \\n\", ratings['rating'].describe())\n",
    "\n",
    "\n",
    "# Import seaborn library\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(font_scale=1.5)\n",
    "%matplotlib inline\n",
    "\n",
    "# Display distribution of rating\n",
    "sns.distplot(ratings['rating'].fillna(ratings['rating'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id gender zipcode  age_desc              occ_desc\n",
      "0        1      F   48067  Under 18          K-12 student\n",
      "1        2      M   70072       56+         self-employed\n",
      "2        3      M   55117     25-34             scientist\n",
      "3        4      M   02460     45-49  executive/managerial\n",
      "4        5      M   55455     25-34                writer\n"
     ]
    }
   ],
   "source": [
    "# Check the top 5 rows\n",
    "print(users.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6040 entries, 0 to 6039\n",
      "Data columns (total 5 columns):\n",
      "user_id     6040 non-null int64\n",
      "gender      6040 non-null object\n",
      "zipcode     6040 non-null object\n",
      "age_desc    6040 non-null object\n",
      "occ_desc    6040 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 236.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check the file info\n",
    "print(users.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id                               title                        genres\n",
      "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4         5  Father of the Bride Part II (1995)                        Comedy\n"
     ]
    }
   ],
   "source": [
    "# Check the top 5 rows\n",
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3883 entries, 0 to 3882\n",
      "Data columns (total 3 columns):\n",
      "movie_id    3883 non-null int64\n",
      "title       3883 non-null object\n",
      "genres      3883 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 91.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check the file info\n",
    "print(movies.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Drama', 1603],\n",
       " ['Comedy', 1200],\n",
       " ['Action', 503],\n",
       " ['Thriller', 492],\n",
       " ['Romance', 471]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading ratings file\n",
    "# Ignore the timestamp column\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "# Join all 3 files into one dataframe\n",
    "dataset = pd.merge(pd.merge(movies, ratings),users)\n",
    "\n",
    "# Display 20 movies with highest ratings\n",
    "dataset[['title','genres','rating']].sort_values('rating', ascending=False).head(20)\n",
    "\n",
    "# Make a census of the genre keywords\n",
    "genre_labels = set()\n",
    "for s in movies['genres'].str.split('|').values:\n",
    "    genre_labels = genre_labels.union(set(s))\n",
    "\n",
    "# Function that counts the number of times each of the genre keywords appear\n",
    "def count_word(dataset, ref_col, census):\n",
    "    keyword_count = dict()\n",
    "    for s in census: \n",
    "        keyword_count[s] = 0\n",
    "    for census_keywords in dataset[ref_col].str.split('|'):        \n",
    "        if type(census_keywords) == float and pd.isnull(census_keywords): \n",
    "            continue        \n",
    "        for s in [s for s in census_keywords if s in census]: \n",
    "            if pd.notnull(s): \n",
    "                keyword_count[s] += 1\n",
    "    #______________________________________________________________________\n",
    "    # convert the dictionary in a list to sort the keywords by frequency\n",
    "    keyword_occurences = []\n",
    "    for k,v in keyword_count.items():\n",
    "        keyword_occurences.append([k,v])\n",
    "    keyword_occurences.sort(key = lambda x:x[1], reverse = True)\n",
    "    return keyword_occurences, keyword_count\n",
    "\n",
    "# Calling this function gives access to a list of genre keywords which are sorted by decreasing frequency\n",
    "keyword_occurences, dum = count_word(movies, 'genres', genre_labels)\n",
    "keyword_occurences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "percent  = 0.2 #0.1 #0.3\n",
    "chosen_idx = np.random.choice(len(users), replace=False, size=int(len(users)*percent))\n",
    "users = users.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(movies), replace=False, size=int(len(movies)*percent))\n",
    "movies = movies.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(ratings), replace=False, size=int(len(ratings)*percent))\n",
    "ratings = ratings.iloc[chosen_idx]\n",
    "\n",
    "#print(user_20.shape, movie_20.shape, ratings_20.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based filtering\n",
    "A Content-Based Recommendation Engine that computes similarity between movies based on movie genres. It will suggest movies that are most similar to a particular movie based on its genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Break up the big genre string into a string array\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "# Convert genres to string value\n",
    "movies['genres'] = movies['genres'].fillna(\"\").astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To judge the machine's performance qualitatively use TfidfVectorizer function from scikit-learn, which transforms text to feature vectors that can be used as input to estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(movies['genres'])\n",
    "tfidf_matrix.shape\n",
    "\n",
    "#Cosine Similarity to calculate a numeric quantity that denotes the similarity between two movies.\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "cosine_sim[:4, :4]\n",
    "\n",
    "# Build a 1-dimensional array with movie titles\n",
    "titles = movies['title']\n",
    "indices = pd.Series(movies.index, index=movies['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that gets movie recommendations based on the cosine similarity score of movie genres. It returns the 20 most \n",
    "similar movies based on the cosine similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_recommendations(title):\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:21]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return titles.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top recommendations for a few movies\n",
    "#genre_recommendations('Good Will Hunting (1997)').head(20)\n",
    "#genre_recommendations('Saving Private Ryan (1998)').head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Fill NaN values in user_id and movie_id column with 0\n",
    "ratings['user_id'] = ratings['user_id'].fillna(0)\n",
    "ratings['movie_id'] = ratings['movie_id'].fillna(0)\n",
    "\n",
    "# Replace NaN values in rating column with average of all values\n",
    "ratings['rating'] = ratings['rating'].fillna(ratings['rating'].mean())\n",
    "\n",
    "#***CHANGING DATASET SIZE for experimentation\n",
    "# Randomly sample 2% of the ratings dataset\n",
    "small_data = ratings.sample(frac=0.02)\n",
    "\n",
    "train_data, test_data = cv.train_test_split(small_data, test_size=0.2)\n",
    "\n",
    "# Create two user-item matrices, one for training and another for testing\n",
    "train_data_matrix = train_data.as_matrix(columns = ['user_id', 'movie_id', 'rating'])\n",
    "test_data_matrix = test_data.as_matrix(columns = ['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "# Check their shape\n",
    "#print(train_data_matrix.shape)\n",
    "#print(test_data_matrix.shape)\n",
    "\n",
    "# User Similarity Matrix\n",
    "user_correlation = 1 - pairwise_distances(train_data, metric='correlation')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "#print(user_correlation[:4, :4])\n",
    "\n",
    "# Item Similarity Matrix\n",
    "item_correlation = 1 - pairwise_distances(train_data_matrix.T, metric='correlation')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "#print(item_correlation[:4, :4])\n",
    "\n",
    "# Function to predict ratings\n",
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        # Use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models\n",
    "\n",
    "The basic idea is that the actual ratings of movies for each user can be represented by a matrix, say of users on the rows and movies along the columns.  \n",
    "We don’t have the full rating matrix, instead, we have a very sparse set of entries. \n",
    "But if we could factor the rating matrix into two separate matrices, say one that was Used by Latent Factors, and one that was Latent Factors by Movies, then we could find the user’s rating for any movie by taking the dot product of the User row and the Movie column.\n",
    "\n",
    "Y = Ratings.\n",
    "X1, X2 = Movies, Users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model 1\n",
    "\n",
    "The user latent features and movie latent features are looked up from the embedding matrices for specific movie-user combination. These are the input values for further linear and non-linear layers. We can pass this input to multiple relu, linear or sigmoid layers and learn the corresponding weights by any optimization algorithm (Adam, SGD, etc.).\n",
    "\n",
    "## The Model\n",
    "- A left embedding layer that creates a Users by Latent Factors matrix.\n",
    "- A right embedding layer that creates a Movies by Latent Factors matrix.\n",
    "- When the input to these layers are (i) a user id and (ii) a movie id, they'll return the latent factor vectors for the user and the movie, respectively.\n",
    "A merge layer that takes the dot product of these two latent vectors to return the predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Merge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-88b6fb0e15a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCFModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Merge'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Embedding, Reshape, Merge\n",
    "from keras.models import Sequential\n",
    "\n",
    "class CFModel(Sequential):\n",
    "\n",
    "    # The constructor for the class\n",
    "    def __init__(self, n_users, m_items, k_factors, **kwargs):\n",
    "        # P is the embedding layer that creates an User by latent factors matrix.\n",
    "        # If the intput is a user_id, P returns the latent factor vector for that user.\n",
    "        P = Sequential()\n",
    "        P.add(Embedding(n_users, k_factors, input_length=1))\n",
    "        P.add(Reshape((k_factors,)))\n",
    "\n",
    "        # Q is the embedding layer that creates a Movie by latent factors matrix.\n",
    "        # If the input is a movie_id, Q returns the latent factor vector for that movie.\n",
    "        Q = Sequential()\n",
    "        Q.add(Embedding(m_items, k_factors, input_length=1))\n",
    "        Q.add(Reshape((k_factors,)))\n",
    "\n",
    "        super(CFModel, self).__init__(**kwargs)\n",
    "        \n",
    "        # The Merge layer takes the dot product of user and movie latent factor vectors to return the corresponding rating.\n",
    "        self.add(Merge([P, Q], mode='dot', dot_axes=1))\n",
    "\n",
    "    # The rate function to predict user's rating of unrated items\n",
    "    def rate(self, user_id, item_id):\n",
    "        return self.predict([np.array([user_id]), np.array([item_id])])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: [4374  838 3588 ... 5819  550 4657] , shape = (200041,)\n",
      "Movies: [ 439 3407 2744 ... 1264  456  312] , shape = (200041,)\n",
      "Ratings: [3 4 5 ... 2 4 2] , shape = (200041,)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "#Loading the datasets again\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'user_emb_id', 'movie_emb_id', 'rating'])\n",
    "max_userid = ratings['user_id'].drop_duplicates().max()\n",
    "max_movieid = ratings['movie_id'].drop_duplicates().max()\n",
    "\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "#Taking 20% of the dataset for epoch experimentation\n",
    "percent  = 0.2\n",
    "chosen_idx = np.random.choice(len(users), replace=False, size=int(len(users)*percent))\n",
    "users = users.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(movies), replace=False, size=int(len(movies)*percent))\n",
    "movies = movies.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(ratings), replace=False, size=int(len(ratings)*percent))\n",
    "ratings = ratings.iloc[chosen_idx]\n",
    "\n",
    "#print(user_20.shape, movie_20.shape, ratings_20.shape)\n",
    "\n",
    "# Create training set\n",
    "shuffled_ratings = ratings.sample(frac=1.)\n",
    "\n",
    "# Shuffling users\n",
    "Users = shuffled_ratings['user_emb_id'].values\n",
    "print('Users:', Users, ', shape =', Users.shape)\n",
    "# Shuffling movies\n",
    "Movies = shuffled_ratings['movie_emb_id'].values\n",
    "print('Movies:', Movies, ', shape =', Movies.shape)\n",
    "# Shuffling ratings\n",
    "Ratings = shuffled_ratings['rating'].values\n",
    "print('Ratings:', Ratings, ', shape =', Ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CFModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-9e5f8b5d8e17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCFModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_userid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_movieid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_FACTORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Compile the model using MSE as the loss function and the AdaMax learning algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adamax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CFModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "K_FACTORS = 100 # The number of dimensional embeddings for movies and users\n",
    "TEST_USER = 2000 # A random test user (user_id = 2000)\n",
    "\n",
    "# Define model\n",
    "model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "# Compile the model using MSE as the loss function and the AdaMax learning algorithm\n",
    "model.compile(loss='mse', optimizer='adamax')\n",
    "\n",
    "# Callbacks monitor the validation loss\n",
    "# Save the model weights each time the validation loss has improved\n",
    "callbacks = [EarlyStopping('val_loss', patience=2), \n",
    "             ModelCheckpoint('weights.h5', save_best_only=True)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHANGE EPOCH NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([Users, Movies], Ratings, nb_epoch=30, validation_split=.1, verbose=2, callbacks=callbacks)\n",
    "\n",
    "###### RMSE\n",
    "# Show the best validation RMSE\n",
    "min_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history.history['val_loss']))\n",
    "print('Minimum RMSE at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(math.sqrt(min_val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CFModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-83829c10e38f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use the pre-trained model, load pre-trained weights from weights.h5 for the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCFModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_userid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_movieid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_FACTORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CFModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the pre-trained model, load pre-trained weights from weights.h5 for the model.\n",
    "trained_model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "# Load weights\n",
    "trained_model.load_weights('weights.h5')\n",
    "\n",
    "# Pick a random test user, random test user is has ID 2000.\n",
    "users[users['user_id'] == TEST_USER]\n",
    "\n",
    "# Function to predict the ratings given User ID and Movie ID\n",
    "def predict_rating(user_id, movie_id):\n",
    "    return trained_model.rate(user_id - 1, movie_id - 1)\n",
    "\n",
    "user_ratings = ratings[ratings['user_id'] == TEST_USER][['user_id', 'movie_id', 'rating']]\n",
    "user_ratings['prediction'] = user_ratings.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
    "user_ratings.sort_values(by='rating', \n",
    "                         ascending=False).merge(movies, \n",
    "                                                on='movie_id', \n",
    "                                                how='inner', \n",
    "                                                suffixes=['_u', '_m']).head(20)\n",
    "\n",
    "recommendations = ratings[ratings['movie_id'].isin(user_ratings['movie_id']) == False][['movie_id']].drop_duplicates()\n",
    "recommendations['prediction'] = recommendations.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
    "recommendations.sort_values(by='prediction',\n",
    "                          ascending=False).merge(movies,\n",
    "                                                 on='movie_id',\n",
    "                                                 how='inner',\n",
    "                                                 suffixes=['_u', '_m']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model 2\n",
    "\n",
    "## The Model\n",
    "Three layer network where the number of neurons per layer decreased geometrically. The input layer is a one-hot encoded input of the user and movie for a single rating. The output is a one-hot encoded output of the score, such that each rating level, incremented by .5, would correspond to an output neuron.\n",
    "\n",
    "Since the neural net attempts to classify which prediction value a user-movie pair should have but doesn’t realize that there’s an inherent relationship between classes, there’s no mechanism to train the model to predict similar ratings when wrong. Training a model that can predict the score using a linear output, rather than several classes may yield better results. Alternatively, rather than just take the output neuron with the highest value, take a weighted average of the neural net’s prediction where the weights are normalized   confidence   estimates   provided   by   the  network.\n",
    "\n",
    "Adopted from https://github.com/alexvlis/movie-recommendation-system.\n",
    "Model presented here but was run locally as matrix file was too large and failed to be uploaded to github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "movieId_movieName = {}\n",
    "movieId_movieCol  = {}\n",
    "userId_userRow    = {}\n",
    "userId_rating     = {}\n",
    "movieId_isRated   = {}\n",
    "\n",
    "'''\n",
    "Read in the movies\n",
    "'''\n",
    "\n",
    "path     = 'data'\n",
    "filename = 'movies.csv'\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "dataFrame = movies\n",
    "\n",
    "for i in range(len(dataFrame['movieId'])):\n",
    "    movieId = dataFrame['movieId'][i]\n",
    "    \n",
    "    movieId_movieName[movieId] = dataFrame['title'][i]    \n",
    "    movieId_isRated[movieId]    = 0\n",
    "   \n",
    "'''\n",
    "Read in the ratings\n",
    "'''\n",
    "path     = 'data'\n",
    "filename = 'ratings.csv'\n",
    "\n",
    "# Reading ratings file\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating','user_emb_id'])\n",
    "dataFrame = ratings\n",
    "\n",
    "for i in range(len(dataFrame)):\n",
    "    userId  = dataFrame['userId'][i]\n",
    "    movieId = dataFrame['movieId'][i]\n",
    "    rating  = dataFrame['rating'][i]\n",
    "    \n",
    "    if userId not in userId_rating.keys():\n",
    "        userId_rating[userId] = [(movieId, rating)]\n",
    "    else:\n",
    "        userId_rating[userId].append((movieId, rating))\n",
    "    \n",
    "    movieId_isRated[movieId] = 1\n",
    "        \n",
    "print(userId_rating[1])\n",
    "    \n",
    "\n",
    "for movieId, isRated in movieId_isRated.items():\n",
    "    if isRated == 0:\n",
    "        del movieId_movieName[movieId]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent  = 0.2 #0.1 #0.3\n",
    "chosen_idx = np.random.choice(len(users), replace=False, size=int(len(users)*percent))\n",
    "userId_rating = users.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(movies), replace=False, size=int(len(movies)*percent))\n",
    "movieId_isRated = movies.iloc[chosen_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the row-column data\n",
    "\n",
    "the rows and columns will have the entries for Ids in sorted order.\n",
    "Therefore:\n",
    "    the ith row    in the data matrix will be the ith key in sorted movieIds\n",
    "    the jth column in the data matrix will be the jth key in sorted userIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId_userRow\n",
    "movieId_movieCol\n",
    "\n",
    "i = 0\n",
    "for movieId in sorted(movieId_movieName):\n",
    "    movieId_movieCol[movieId] = i\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for userId in sorted(userId_rating):\n",
    "    userId_userRow[userId] = i\n",
    "    i+=1\n",
    "\n",
    "m = len(userId_userRow.keys())\n",
    "n = len(movieId_movieCol.keys())\n",
    "A = np.zeros((m,n))\n",
    "\n",
    "\n",
    "    \n",
    "print(A.shape)\n",
    "for userId, ratings in userId_rating.items():\n",
    "    for rating in ratings:\n",
    "        movieId   = rating[0]\n",
    "        score     = rating[1]\n",
    "        \n",
    "        if (userId in userId_userRow and movieId in movieId_movieCol):\n",
    "            i = userId_userRow[userId]\n",
    "\n",
    "            j = movieId_movieCol[movieId]\n",
    "            A[i,j] = score\n",
    "\n",
    "ratingCount = 0\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        if (A[i][j] != 0):\n",
    "#             if(ratingCount < 20):\n",
    "#                 print(A[i][j])\n",
    "            ratingCount += 1\n",
    "\n",
    "\n",
    "print('Number of ratings = {}'.format(ratingCount))\n",
    "print('Total entries = {}'.format(m*n))\n",
    "print('Sparsity = {}%'.format(ratingCount*100/(m*n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "write the relevant dictionaries and matrix to a file\n",
    "'''\n",
    "# movieId_movieName\n",
    "# movieId_movieCol\n",
    "# userId_userRow\n",
    "# userId_rating\n",
    "# A\n",
    "\n",
    "d = {'movieId_movieName': movieId_movieName,\n",
    "     'movieId_movieCol' : movieId_movieCol,\n",
    "     'userId_userRow'   : userId_userRow,\n",
    "     'userId_rating'    : userId_rating }\n",
    "pickle.dump(A, open('data/data_matrix.p', 'wb'))\n",
    "pickle.dump(d, open('data/data_dicts.p', 'wb'))\n",
    "print (A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Model\n",
    "\n",
    "Constructs a Neural network with a given pattern.\n",
    "The pattern indicates how many neurons should exist at every layer.\n",
    "Param:\n",
    "    hidden_layer_pattern - The input layer and output layer are fixed, but the rate at which the layer sizes\n",
    "    decreases depends on the parameter, hidden_layer_pattern.\n",
    "    \n",
    "***Could be further experimented by changing the number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class Collaborative_Filtering_Neural_Net(object):\n",
    "\tdef __init__(self, train_data, val_data, mask, num_layers=3, learn_rate=.2):\n",
    "\t\tself.train_data = train_data\n",
    "\t\tself.val_data   = val_data\n",
    "\t\tself.mask       = mask\n",
    "\t\tself.num_layers = num_layers\n",
    "\t\tself.m          = self.train_data.shape[0]\n",
    "\t\tself.n \t\t\t= self.train_data.shape[1]\n",
    "\t\tself.learn_rate = learn_rate\n",
    "\t\tself.construct_input()\n",
    "\n",
    "\tdef construct_input(self):\n",
    "\t\tdef change_to_one_hot(value, value_range):\n",
    "\t\t\tone_hot_vec = np.zeros(len(value_range))\n",
    "\t\t\tone_hot_vec[int(value/.5)] = 1\n",
    "\t\t\treturn one_hot_vec\n",
    "\n",
    "\t\tm = self.m\n",
    "\t\tn = self.n\n",
    "\n",
    "\t\tuser_indices, movie_indices = (np.where(self.train_data > 0))\n",
    "\t\tscores = self.train_data[self.mask]\n",
    "\n",
    "\t\tnum_train_samples = user_indices.shape[0]\n",
    "\n",
    "\t\tself.train_x = np.zeros((num_train_samples, m+n))\n",
    "\t\tself.train_y = np.zeros((num_train_samples, 11))\n",
    "\n",
    "\t\tstart = time.time()\n",
    "\n",
    "\t\t#construct training input and output X, y\n",
    "\t\tfor i in range(num_train_samples):\n",
    "\t\t\tu_ind = user_indices[i]\n",
    "\t\t\tm_ind = movie_indices[i]\n",
    "\n",
    "\t\t\tself.train_x[i, u_ind]   = 1\n",
    "\t\t\tself.train_x[i, m+m_ind] = 1\n",
    "\n",
    "\t\t\tscore \t\t\t= self.train_data[u_ind, m_ind]\n",
    "\t\t\tself.train_y[i] = change_to_one_hot(score, np.arange(0,5.5,.5))\n",
    "\n",
    "\t\t#construct test inputs for where we need to predict values\n",
    "\t\tuser_indices, movie_indices = np.where(self.mask)\n",
    "\t\tnum_test_samples = user_indices.shape[0]\n",
    "\t\tself.test_x = np.zeros((num_test_samples, m+n))\n",
    "\t\tself.test_y = np.zeros((num_test_samples, 11))\n",
    "\n",
    "\t\tfor i in range(num_test_samples):\n",
    "\t\t\tu_ind = user_indices[i]\n",
    "\t\t\tm_ind = movie_indices[i]\n",
    "\n",
    "\t\t\tself.test_x[i, u_ind]   = 1\n",
    "\t\t\tself.test_x[i, m+m_ind] = 1\n",
    "\n",
    "\t\t\tscore \t\t   = self.val_data[u_ind, m_ind]\n",
    "\t\t\tself.test_y[i] = change_to_one_hot(score, np.arange(0,5.5,.5))\n",
    "\n",
    "\t\tprint(time.time() - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef construct_model(self, hidden_layer_pattern = 'exponential'):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinput_size = self.m + self.n\n",
    "\t\t\n",
    "\t\t# add the first layer\n",
    "\t\tmodel.add(Dense(input_size, activation='relu', input_shape=(input_size,)))\n",
    "\n",
    "\t\t#one of the two model architectures tested\n",
    "\t\tif (hidden_layer_pattern == 'linear'):\n",
    "\t\t\tlinear_decrease = int(input_size/self.num_layers)\n",
    "\t\t\tfor i in range(self.num_layers):\n",
    "\t\t\t\tinput_size = input_size - linear_decrease\n",
    "\t\t\t\tmodel.add(Dense(input_size, activation='relu') )\n",
    "\n",
    "\t\tif (hidden_layer_pattern == 'exponential'):\n",
    "\t\t\texponential_decrease = int((np.exp(np.log(input_size)/(self.num_layers+2))))\n",
    "\t\t\tprint(exponential_decrease)\n",
    "\t\t\tfor i in range(self.num_layers):\n",
    "\t\t\t\tinput_size = int(input_size/exponential_decrease);\n",
    "\t\t\t\tmodel.add(Dense(input_size, activation='relu') )\n",
    "\n",
    "\t\tprint (model.output_shape)\n",
    "        \n",
    "\t\t#one hot encoded output\n",
    "\t\tmodel.add(Dense(11, activation='relu'))\n",
    "\n",
    "\t\tadam = optimizers.Adam(lr=self.learn_rate, decay=.001)\n",
    "\t\tmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\t\tself.model = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and train Model\n",
    "\n",
    "Trains the model. Saves checkpoints of the model at every epoch.\n",
    "Param:\n",
    "    model_number - Just changes the filename that the model is saved to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(self, model_number = 0):\n",
    "\n",
    "\t\t# lets make checkpoints\n",
    "\t\tfilepath = \"nn_model_{}_lr_{}\".format(model_number, self.learn_rate)\n",
    "\t\tfilepath+= \"_{epoch:02d}.hdf5\"\n",
    "\n",
    "\t\tprint('learn_rate = {}'.format(self.learn_rate))\n",
    "\t\tcheckpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\t\tcallbacks_list = [checkpoint]\n",
    "\n",
    "        #***CHANGE EPOCH NUMBER HERE\n",
    "\t\tself.model.fit(self.train_x, self.train_y, batch_size=128, epochs=20, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\tdef load_model(self, filename):\n",
    "\t\t'''\n",
    "\t\tLoads the weights of an identically architectured neural net at the given filepath\n",
    "\t\t'''\n",
    "\t\tself.model.load_weights(filename)\n",
    "\t\tadam = optimizers.Adam(lr=self.learn_rate, decay=.001)\n",
    "\t\tself.model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT\n",
    "Predicts values based on training or validation data\n",
    "Return:\n",
    "    scores\n",
    "    predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_values(self, test_type='validation'):\n",
    "    # print(self.model.get_weights())\n",
    "    if (test_type == 'validation'):\n",
    "        scores = self.model.predict(self.test_x, verbose=True)\n",
    "        return scores, self.test_y\n",
    "    elif (test_type == 'training'):\n",
    "        scores = self.model.predict(self.train_x, verbose=True)\n",
    "        return scores, self.train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN AND TEST\n",
    "\n",
    "Trains a neural net and saves snapshots every epoch. Runs 20 epochs or until you quit the process. Gets the accuracy and validation error of a model. This function assumes you have been saving your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import util\n",
    "from nn import Collaborative_Filtering_Neural_Net\n",
    "\n",
    "def train():\n",
    "\ttrain_mat, val_mat, masks = util.k_cross()\n",
    "\tA = util.load_data_matrix()\n",
    "\n",
    "\tstart = time.time()\n",
    "\tnet = Collaborative_Filtering_Neural_Net(train_mat[0], val_mat[0], masks[0])\n",
    "\tnet.learn_rate=.1\n",
    "\tnet.construct_model(hidden_layer_pattern = 'exponential')\n",
    "\t# net.load_model('nn_model_exponential_one_hot_learn_rate_.1_lr_0.1_04.hdf5')\n",
    "\tnet.train_model(model_number='exponential_one_hot')\n",
    "\tprint('time taken to train in seconds:', time.time() - start)\n",
    "\n",
    "def test(model_name = '', test_type = 'validation'):\n",
    "\ttrain_mat, val_mat, masks = util.k_cross()\n",
    "\tA = util.load_data_matrix()\n",
    "\n",
    "\tnet = Collaborative_Filtering_Neural_Net(train_mat[0], val_mat[0], masks[0])\n",
    "\tnet.learn_rate=.1\n",
    "\tnet.construct_model(hidden_layer_pattern = 'exponential')\n",
    "\tnet.load_model(model_name)\n",
    "\n",
    "\tpred_scores , true_scores= net.predict_values(test_type = test_type)\n",
    "\tpred_scores = pred_scores.argmax(axis=1)\n",
    "\ttrue_scores    = true_scores.argmax(axis=1)\n",
    "\n",
    "\t#get Accuracy\n",
    "\tnum_correct = np.sum(pred_scores == true_scores)\n",
    "\taccuracy    = num_correct/pred_scores.shape[0]*100\n",
    "\n",
    "\t#get MSE\n",
    "\terror = pred_scores-true_scores\n",
    "\tmse   = np.mean(np.power(error, 2))\n",
    "\n",
    "\tprint('The {} accuracy of the model is {}%'.format(test_type, accuracy))\n",
    "\tprint('The {} mean squared error of the model is {}'.format(test_type, mse))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttrain()\n",
    "\ttest('nn_model_exponential_one_hot_round_2_lr_0.1_08.hdf5', test_type='training')\n",
    "\ttest('nn_model_exponential_one_hot_round_2_lr_0.1_08.hdf5', test_type='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility\n",
    "\n",
    "Create the training and validation matrices for k_cross validaton\n",
    "\tparam:\n",
    "\t\tfilename - name of pickle file with original (m x n) data matrix, must have file extension\n",
    "\t\tpath     - path to the filename - can be omitted if path is included in filename\n",
    "\t\tk        - number of training/data sets\n",
    "\treturns:\n",
    "\t\ttraining_matrices   - array of (m x n) training matrices\n",
    "\t\tprediction_matrices - array of (m x n) validation matrices where each data point is omitted from the corresponding \n",
    "\t\t\t\t\t\t\t  training matrix\n",
    "\t\tindex_matrices      - array of (m x n) boolean masks where each true element is where an element was transplanted\n",
    "\t\t\t\t\t\t\t  from the training matrix to the validation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "def k_cross(k = 10):\n",
    "\n",
    "\tA = load_data_matrix()\n",
    "\tm = A.shape[0]\n",
    "\tn = A.shape[1]\n",
    "\n",
    "\tprint('A.shape = {}'.format(A.shape))\n",
    "\n",
    "\tprediction_matrices = []\n",
    "\ttraining_matrices   = []\n",
    "\tindex_lists         = []\n",
    "\tfor i in range(k):\n",
    "\t    A_copy = A.copy()\n",
    "\t    prediction_matrices.append(np.zeros((m, n)))\n",
    "\t    training_matrices.append(A_copy)\n",
    "\t    index_lists.append(np.zeros((m, n), dtype=bool))\n",
    "\n",
    "\tit    = 0\n",
    "\tfor i in range(A.shape[0]):\n",
    "\t    for j in range(A.shape[1]):\n",
    "\t        if (A[i, j] != 0):\n",
    "\t            training_matrices[it%k][i, j]   = 0\n",
    "\t            prediction_matrices[it%k][i, j] = A[i, j]\n",
    "\t            index_lists[it%k][i, j] = True\n",
    "\t            it+=1\n",
    "\n",
    "\treturn training_matrices, prediction_matrices, index_lists\n",
    "\n",
    "def load_data_matrix(filename='data_matrix.p', path='data'):\n",
    "\tfilepath = filename if path == '' else '{}/{}'.format(path,filename)\n",
    "\tA = pickle.load( open('{}'.format(filepath), 'rb'))\n",
    "\treturn A\n",
    "\n",
    "def get_MSE(mat1, mask, mat2=''):\n",
    "\tif (mat2 == ''):\n",
    "\t\tmat2 = load_data_matrix()\n",
    "\n",
    "\tA_mask    = mat2[mask]\n",
    "\tmat1_mask = mat1[mask]\n",
    "\n",
    "\tdiff = A_mask-mat1_mask\n",
    "\tmse = np.dot(diff, diff)/A_mask.shape\n",
    "\treturn mse[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tk = 10\n",
    "\n",
    "\ttrain_mats, val_mats, masks = k_cross(k=k)\n",
    "\tprint('MSE = {}'.format(get_MSE(train_mats[0], masks[0])))\n",
    "\n",
    "\tm = train_mats[0].shape[0]\n",
    "\tn = train_mats[0].shape[1]\n",
    "\tstart = time.time()\n",
    "\tfor i in range(m):\n",
    "\t    for j in range(n):\n",
    "\t        for index in range(k):\n",
    "\t            if(train_mats[index][i,j] != 0 and val_mats[index][i,j] != 0):\n",
    "\t                print('we have a problem')\n",
    "\tend = time.time()\n",
    "\tprint('you wasted {} seconds of my life'.format(end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
