{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation \n",
    "\n",
    "\n",
    "## The MovieLens Dataset\n",
    "Contains 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((6040, 5), 6040)\n",
      "((3883, 3), 3883)\n",
      "((1000209, 4), 1000209)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading ratings file\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating','user_emb_id'])\n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "print(users.shape, len(users))\n",
    "print(movies.shape, len(movies))\n",
    "print(ratings.shape, len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id  rating  user_emb_id\n",
      "0        1      1193       5            0\n",
      "1        1       661       3            0\n",
      "2        1       914       3            0\n",
      "3        1      3408       4            0\n",
      "4        1      2355       5            0\n"
     ]
    }
   ],
   "source": [
    "# Check the top 5 rows\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000209 entries, 0 to 1000208\n",
      "Data columns (total 4 columns):\n",
      "user_id        1000209 non-null int64\n",
      "movie_id       1000209 non-null int64\n",
      "rating         1000209 non-null int64\n",
      "user_emb_id    1000209 non-null int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 30.5 MB\n",
      "None\n",
      "('Describe Ratings: \\n', count    1.000209e+06\n",
      "mean     3.581564e+00\n",
      "std      1.117102e+00\n",
      "min      1.000000e+00\n",
      "25%      3.000000e+00\n",
      "50%      4.000000e+00\n",
      "75%      4.000000e+00\n",
      "max      5.000000e+00\n",
      "Name: rating, dtype: float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f671114e250>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6711140b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the file info\n",
    "print(ratings.info())\n",
    "# Get summary statistics of rating\n",
    "print(\"Describe Ratings: \\n\", ratings['rating'].describe())\n",
    "\n",
    "\n",
    "# Import seaborn library\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(font_scale=1.5)\n",
    "%matplotlib inline\n",
    "\n",
    "# Display distribution of rating\n",
    "sns.distplot(ratings['rating'].fillna(ratings['rating'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id gender zipcode  age_desc              occ_desc\n",
      "0        1      F   48067  Under 18          K-12 student\n",
      "1        2      M   70072       56+         self-employed\n",
      "2        3      M   55117     25-34             scientist\n",
      "3        4      M   02460     45-49  executive/managerial\n",
      "4        5      M   55455     25-34                writer\n"
     ]
    }
   ],
   "source": [
    "# Check the top 5 rows\n",
    "print(users.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id                               title                        genres\n",
      "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4         5  Father of the Bride Part II (1995)                        Comedy\n"
     ]
    }
   ],
   "source": [
    "# Check the top 5 rows\n",
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3883 entries, 0 to 3882\n",
      "Data columns (total 3 columns):\n",
      "movie_id    3883 non-null int64\n",
      "title       3883 non-null object\n",
      "genres      3883 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 91.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check the file info\n",
    "print(movies.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Drama', 1603],\n",
       " [u'Comedy', 1200],\n",
       " [u'Action', 503],\n",
       " [u'Thriller', 492],\n",
       " [u'Romance', 471]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading ratings file\n",
    "# Ignore the timestamp column\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "# Join all 3 files into one dataframe\n",
    "dataset = pd.merge(pd.merge(movies, ratings),users)\n",
    "\n",
    "# Display 20 movies with highest ratings\n",
    "dataset[['title','genres','rating']].sort_values('rating', ascending=False).head(20)\n",
    "\n",
    "# Make a census of the genre keywords\n",
    "genre_labels = set()\n",
    "for s in movies['genres'].str.split('|').values:\n",
    "    genre_labels = genre_labels.union(set(s))\n",
    "\n",
    "# Function that counts the number of times each of the genre keywords appear\n",
    "def count_word(dataset, ref_col, census):\n",
    "    keyword_count = dict()\n",
    "    for s in census: \n",
    "        keyword_count[s] = 0\n",
    "    for census_keywords in dataset[ref_col].str.split('|'):        \n",
    "        if type(census_keywords) == float and pd.isnull(census_keywords): \n",
    "            continue        \n",
    "        for s in [s for s in census_keywords if s in census]: \n",
    "            if pd.notnull(s): \n",
    "                keyword_count[s] += 1\n",
    "    #______________________________________________________________________\n",
    "    # convert the dictionary in a list to sort the keywords by frequency\n",
    "    keyword_occurences = []\n",
    "    for k,v in keyword_count.items():\n",
    "        keyword_occurences.append([k,v])\n",
    "    keyword_occurences.sort(key = lambda x:x[1], reverse = True)\n",
    "    return keyword_occurences, keyword_count\n",
    "\n",
    "# Calling this function gives access to a list of genre keywords which are sorted by decreasing frequency\n",
    "keyword_occurences, dum = count_word(movies, 'genres', genre_labels)\n",
    "keyword_occurences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based filtering\n",
    "A Content-Based Recommendation Engine that computes similarity between movies based on movie genres. It will suggest movies that are most similar to a particular movie based on its genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deb/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Break up the big genre string into a string array\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "# Convert genres to string value\n",
    "movies['genres'] = movies['genres'].fillna(\"\").astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To judge the machine's performance qualitatively use TfidfVectorizer function from scikit-learn, which transforms text to feature vectors that can be used as input to estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(movies['genres'])\n",
    "tfidf_matrix.shape\n",
    "\n",
    "#Cosine Similarity to calculate a numeric quantity that denotes the similarity between two movies.\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "cosine_sim[:4, :4]\n",
    "\n",
    "# Build a 1-dimensional array with movie titles\n",
    "titles = movies['title']\n",
    "indices = pd.Series(movies.index, index=movies['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that gets movie recommendations based on the cosine similarity score of movie genres. It returns the 20 most \n",
    "similar movies based on the cosine similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_recommendations(title):\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:21]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return titles.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top recommendations for a few movies\n",
    "#genre_recommendations('Good Will Hunting (1997)').head(20)\n",
    "#genre_recommendations('Saving Private Ryan (1998)').head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two collaborative filtering approaches: Memory-based approach and Model-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/2000/1*7uW5hLXztSu_FOmZOWpB6g.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://cdn-images-1.medium.com/max/2000/1*7uW5hLXztSu_FOmZOWpB6g.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Memory based approach: \n",
    "### User-based filtering and Item-based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1000209, 3), (1000, 3))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "percent  = 0.001  #0.1 #0.3\n",
    "chosen_idx = np.random.choice(len(users), replace=False, size=int(len(users)*percent))\n",
    "users = users.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(movies), replace=False, size=int(len(movies)*percent))\n",
    "movies = movies.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(ratings), replace=False, size=int(len(ratings)*percent))\n",
    "small_data = ratings.iloc[chosen_idx]\n",
    "\n",
    "print(ratings.shape,small_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deb/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/deb/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/home/deb/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1)\n",
      "(200, 1)\n",
      "User-based CF RMSE: 1.65529453572\n",
      "Item-based CF RMSE: 1.65529453572\n",
      "User-based CF RMSE: 0.0\n",
      "Item-based CF RMSE: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Fill NaN values in user_id and movie_id column with 0\n",
    "\n",
    "small_data['user_id'] = small_data['user_id'].fillna(0)\n",
    "small_data['movie_id'] = small_data['movie_id'].fillna(0)\n",
    "\n",
    "# Replace NaN values in rating column with average of all values\n",
    "small_data['rating'] = small_data['rating'].fillna(small_data['rating'].mean())\n",
    "\n",
    "train_data, test_data = cv.train_test_split(small_data, test_size=0.2)\n",
    "\n",
    "# Create two user-item matrices, one for training and another for testing\n",
    "train_data_matrix = train_data.as_matrix(columns = ['rating'])\n",
    "test_data_matrix = test_data.as_matrix(columns = ['rating'])\n",
    "\n",
    "# Check their shape\n",
    "print(train_data_matrix.shape)\n",
    "print(test_data_matrix.shape)\n",
    "\n",
    "# User Similarity Matrix\n",
    "user_correlation = 1 - pairwise_distances(train_data, metric='correlation')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "#print(user_correlation[:4, :4])\n",
    "\n",
    "# Item Similarity Matrix\n",
    "item_correlation = 1 - pairwise_distances(train_data_matrix.T, metric='correlation')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "#print(item_correlation[:4, :4])\n",
    "\n",
    "# Function to predict ratings\n",
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        # Use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(pred, actual))\n",
    "# Predict ratings on the training data with both similarity score\n",
    "user_prediction = predict(train_data_matrix, user_correlation, type='user')\n",
    "item_prediction = predict(train_data_matrix, item_correlation, type='item')\n",
    "\n",
    "# RMSE on the test data\n",
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix)))\n",
    "# RMSE on the train data\n",
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, train_data_matrix)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, train_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models\n",
    "\n",
    "The basic idea is that the actual ratings of movies for each user can be represented by a matrix, say of users on the rows and movies along the columns.  \n",
    "We don’t have the full rating matrix, instead, we have a very sparse set of entries. \n",
    "But if we could factor the rating matrix into two separate matrices, say one that was Used by Latent Factors, and one that was Latent Factors by Movies, then we could find the user’s rating for any movie by taking the dot product of the User row and the Movie column.\n",
    "\n",
    "Y = Ratings.\n",
    "X1, X2 = Movies, Users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model 1\n",
    "\n",
    "The user latent features and movie latent features are looked up from the embedding matrices for specific movie-user combination. These are the input values for further linear and non-linear layers. We can pass this input to multiple relu, linear or sigmoid layers and learn the corresponding weights by any optimization algorithm (Adam, SGD, etc.).\n",
    "\n",
    "## The Model\n",
    "- A left embedding layer that creates a Users by Latent Factors matrix.\n",
    "- A right embedding layer that creates a Movies by Latent Factors matrix.\n",
    "- When the input to these layers are (i) a user id and (ii) a movie id, they'll return the latent factor vectors for the user and the movie, respectively.\n",
    "A merge layer that takes the dot product of these two latent vectors to return the predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deb/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Embedding, Reshape, Merge\n",
    "from keras.models import Sequential\n",
    "\n",
    "class CFModel(Sequential):\n",
    "\n",
    "    # The constructor for the class\n",
    "    def __init__(self, n_users, m_items, k_factors, **kwargs):\n",
    "        # P is the embedding layer that creates an User by latent factors matrix.\n",
    "        # If the intput is a user_id, P returns the latent factor vector for that user.\n",
    "        P = Sequential()\n",
    "        P.add(Embedding(n_users, k_factors, input_length=1))\n",
    "        P.add(Reshape((k_factors,)))\n",
    "\n",
    "        # Q is the embedding layer that creates a Movie by latent factors matrix.\n",
    "        # If the input is a movie_id, Q returns the latent factor vector for that movie.\n",
    "        Q = Sequential()\n",
    "        Q.add(Embedding(m_items, k_factors, input_length=1))\n",
    "        Q.add(Reshape((k_factors,)))\n",
    "\n",
    "        super(CFModel, self).__init__(**kwargs)\n",
    "        \n",
    "        # The Merge layer takes the dot product of user and movie latent factor vectors to return the corresponding rating.\n",
    "        self.add(Merge([P, Q], mode='dot', dot_axes=1))\n",
    "\n",
    "    # The rate function to predict user's rating of unrated items\n",
    "    def rate(self, user_id, item_id):\n",
    "        return self.predict([np.array([user_id]), np.array([item_id])])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Users:', array([4226, 1009, 3295, ...,  638, 4844, 2392]), ', shape =', (200041,))\n",
      "('Movies:', array([ 247,  367, 1999, ..., 2054, 1065, 2012]), ', shape =', (200041,))\n",
      "('Ratings:', array([2, 4, 5, ..., 4, 4, 4]), ', shape =', (200041,))\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "#Loading the datasets again\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'user_emb_id', 'movie_emb_id', 'rating'])\n",
    "max_userid = ratings['user_id'].drop_duplicates().max()\n",
    "max_movieid = ratings['movie_id'].drop_duplicates().max()\n",
    "\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "#Taking 20% of the dataset for epoch experimentation\n",
    "percent  = 0.2\n",
    "chosen_idx = np.random.choice(len(users), replace=False, size=int(len(users)*percent))\n",
    "users = users.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(movies), replace=False, size=int(len(movies)*percent))\n",
    "movies = movies.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(ratings), replace=False, size=int(len(ratings)*percent))\n",
    "ratings = ratings.iloc[chosen_idx]\n",
    "\n",
    "#print(user_20.shape, movie_20.shape, ratings_20.shape)\n",
    "\n",
    "# Create training set\n",
    "shuffled_ratings = ratings.sample(frac=1.)\n",
    "\n",
    "# Shuffling users\n",
    "Users = shuffled_ratings['user_emb_id'].values\n",
    "print('Users:', Users, ', shape =', Users.shape)\n",
    "# Shuffling movies\n",
    "Movies = shuffled_ratings['movie_emb_id'].values\n",
    "print('Movies:', Movies, ', shape =', Movies.shape)\n",
    "# Shuffling ratings\n",
    "Ratings = shuffled_ratings['rating'].values\n",
    "print('Ratings:', Ratings, ', shape =', Ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deb/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:24: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "K_FACTORS = 100 # The number of dimensional embeddings for movies and users\n",
    "\n",
    "# Define model\n",
    "model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "# Compile the model using MSE as the loss function and the AdaMax learning algorithm\n",
    "model.compile(loss='mse', optimizer='adamax')\n",
    "\n",
    "# Callbacks monitor the validation loss\n",
    "# Save the model weights each time the validation loss has improved\n",
    "callbacks = [EarlyStopping('val_loss', patience=2), \n",
    "             ModelCheckpoint('weights.h5', save_best_only=True)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHANGE EPOCH NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deb/anaconda2/lib/python2.7/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180036 samples, validate on 20005 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ea533311015f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUsers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMovies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRatings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m###### RMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Show the best validation RMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmin_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deb/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/deb/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/deb/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deb/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deb/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deb/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deb/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deb/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deb/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deb/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit([Users, Movies], Ratings, nb_epoch=30, validation_split=.1, verbose=2, callbacks=callbacks)\n",
    "\n",
    "###### RMSE\n",
    "# Show the best validation RMSE\n",
    "min_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history.history['val_loss']))\n",
    "print('Minimum RMSE at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(math.sqrt(min_val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deb/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:24: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate 'str' and 'int' objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-75e8a18a5dc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mTEST_USER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserI\u001b[0m \u001b[0;31m# A random test user (user_id = 2000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTEST_USER\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTEST_USER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0muser_ratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTEST_USER\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'movie_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0muser_ratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_ratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredict_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_USER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'movie_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate 'str' and 'int' objects"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the pre-trained model, load pre-trained weights from weights.h5 for the model.\n",
    "trained_model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "# Load weights\n",
    "trained_model.load_weights('weights.h5')\n",
    "\n",
    "# Pick a random test user, random test user is has ID 2000.\n",
    "\n",
    "\n",
    "\n",
    "# Function to predict the ratings given User ID and Movie ID\n",
    "def predict_rating(user_id, movie_id):\n",
    "    return trained_model.rate(user_id - 1, movie_id - 1)\n",
    "\n",
    "final = {}\n",
    "for userI in range(1, 6044):\n",
    "    TEST_USER = userI # A random test user (user_id = 2000)\n",
    "    users[users['user_id'] == TEST_USER]\n",
    "    print(\"User\" + TEST_USER)\n",
    "    user_ratings = ratings[ratings['user_id'] == TEST_USER][['user_id', 'movie_id', 'rating']]\n",
    "    user_ratings['prediction'] = user_ratings.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
    "    user_ratings.sort_values(by='rating', \n",
    "                         ascending=False).merge(movies, \n",
    "                                                on='movie_id', \n",
    "                                                how='inner', \n",
    "                                                suffixes=['_u', '_m']).head(20)\n",
    "    print(user_ratings)\n",
    "\n",
    "    recommendations = ratings[ratings['movie_id'].isin(user_ratings['movie_id']) == False][['movie_id']].drop_duplicates()\n",
    "    recommendations['prediction'] = recommendations.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
    "    recommendations.sort_values(by='prediction',\n",
    "                          ascending=False).merge(movies,\n",
    "                                                 on='movie_id',\n",
    "                                                 how='inner',\n",
    "                                                 suffixes=['_u', '_m']).head(20)\n",
    "    print(recommendations)\n",
    "    #final.append(user_ratings)\n",
    "    #final.append(recommendations)\n",
    "    #FinalData = user_ratings + recommendations\n",
    "    #print(FinalData)\n",
    "    \n",
    "#FinalData.to_csv('recPred.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('model Mae')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model 2\n",
    "\n",
    "## The Model\n",
    "Three layer network where the number of neurons per layer decreased geometrically. The input layer is a one-hot encoded input of the user and movie for a single rating. The output is a one-hot encoded output of the score, such that each rating level, incremented by .5, would correspond to an output neuron.\n",
    "\n",
    "Since the neural net attempts to classify which prediction value a user-movie pair should have but doesn’t realize that there’s an inherent relationship between classes, there’s no mechanism to train the model to predict similar ratings when wrong. Training a model that can predict the score using a linear output, rather than several classes may yield better results. Alternatively, rather than just take the output neuron with the highest value, take a weighted average of the neural net’s prediction where the weights are normalized   confidence   estimates   provided   by   the  network.\n",
    "\n",
    "Adopted from https://github.com/alexvlis/movie-recommendation-system.\n",
    "Model presented here but was run locally as matrix file was too large and failed to be uploaded to github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "movieId_movieName = {}\n",
    "movieId_movieCol  = {}\n",
    "userId_userRow    = {}\n",
    "userId_rating     = {}\n",
    "movieId_isRated   = {}\n",
    "\n",
    "'''\n",
    "Read in the movies\n",
    "'''\n",
    "\n",
    "path     = 'data'\n",
    "filename = 'movies.csv'\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "dataFrame = movies\n",
    "\n",
    "for i in range(len(dataFrame['movieId'])):\n",
    "    movieId = dataFrame['movieId'][i]\n",
    "    \n",
    "    movieId_movieName[movieId] = dataFrame['title'][i]    \n",
    "    movieId_isRated[movieId]    = 0\n",
    "   \n",
    "'''\n",
    "Read in the ratings\n",
    "'''\n",
    "path     = 'data'\n",
    "filename = 'ratings.csv'\n",
    "\n",
    "# Reading ratings file\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating','user_emb_id'])\n",
    "dataFrame = ratings\n",
    "\n",
    "for i in range(len(dataFrame)):\n",
    "    userId  = dataFrame['userId'][i]\n",
    "    movieId = dataFrame['movieId'][i]\n",
    "    rating  = dataFrame['rating'][i]\n",
    "    \n",
    "    if userId not in userId_rating.keys():\n",
    "        userId_rating[userId] = [(movieId, rating)]\n",
    "    else:\n",
    "        userId_rating[userId].append((movieId, rating))\n",
    "    \n",
    "    movieId_isRated[movieId] = 1\n",
    "        \n",
    "print(userId_rating[1])\n",
    "    \n",
    "\n",
    "for movieId, isRated in movieId_isRated.items():\n",
    "    if isRated == 0:\n",
    "        del movieId_movieName[movieId]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent  = 0.2 #0.1 #0.3\n",
    "chosen_idx = np.random.choice(len(users), replace=False, size=int(len(users)*percent))\n",
    "userId_rating = users.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(movies), replace=False, size=int(len(movies)*percent))\n",
    "movieId_isRated = movies.iloc[chosen_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the row-column data\n",
    "\n",
    "the rows and columns will have the entries for Ids in sorted order.\n",
    "Therefore:\n",
    "    the ith row    in the data matrix will be the ith key in sorted movieIds\n",
    "    the jth column in the data matrix will be the jth key in sorted userIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId_userRow\n",
    "movieId_movieCol\n",
    "\n",
    "i = 0\n",
    "for movieId in sorted(movieId_movieName):\n",
    "    movieId_movieCol[movieId] = i\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for userId in sorted(userId_rating):\n",
    "    userId_userRow[userId] = i\n",
    "    i+=1\n",
    "\n",
    "m = len(userId_userRow.keys())\n",
    "n = len(movieId_movieCol.keys())\n",
    "A = np.zeros((m,n))\n",
    "\n",
    "\n",
    "    \n",
    "print(A.shape)\n",
    "for userId, ratings in userId_rating.items():\n",
    "    for rating in ratings:\n",
    "        movieId   = rating[0]\n",
    "        score     = rating[1]\n",
    "        \n",
    "        if (userId in userId_userRow and movieId in movieId_movieCol):\n",
    "            i = userId_userRow[userId]\n",
    "\n",
    "            j = movieId_movieCol[movieId]\n",
    "            A[i,j] = score\n",
    "\n",
    "ratingCount = 0\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        if (A[i][j] != 0):\n",
    "#             if(ratingCount < 20):\n",
    "#                 print(A[i][j])\n",
    "            ratingCount += 1\n",
    "\n",
    "\n",
    "print('Number of ratings = {}'.format(ratingCount))\n",
    "print('Total entries = {}'.format(m*n))\n",
    "print('Sparsity = {}%'.format(ratingCount*100/(m*n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "write the relevant dictionaries and matrix to a file\n",
    "'''\n",
    "# movieId_movieName\n",
    "# movieId_movieCol\n",
    "# userId_userRow\n",
    "# userId_rating\n",
    "# A\n",
    "\n",
    "d = {'movieId_movieName': movieId_movieName,\n",
    "     'movieId_movieCol' : movieId_movieCol,\n",
    "     'userId_userRow'   : userId_userRow,\n",
    "     'userId_rating'    : userId_rating }\n",
    "pickle.dump(A, open('data/data_matrix.p', 'wb'))\n",
    "pickle.dump(d, open('data/data_dicts.p', 'wb'))\n",
    "print (A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Model\n",
    "\n",
    "Constructs a Neural network with a given pattern.\n",
    "The pattern indicates how many neurons should exist at every layer.\n",
    "Param:\n",
    "    hidden_layer_pattern - The input layer and output layer are fixed, but the rate at which the layer sizes\n",
    "    decreases depends on the parameter, hidden_layer_pattern.\n",
    "    \n",
    "***Could be further experimented by changing the number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class Collaborative_Filtering_Neural_Net(object):\n",
    "\tdef __init__(self, train_data, val_data, mask, num_layers=3, learn_rate=.2):\n",
    "\t\tself.train_data = train_data\n",
    "\t\tself.val_data   = val_data\n",
    "\t\tself.mask       = mask\n",
    "\t\tself.num_layers = num_layers\n",
    "\t\tself.m          = self.train_data.shape[0]\n",
    "\t\tself.n \t\t\t= self.train_data.shape[1]\n",
    "\t\tself.learn_rate = learn_rate\n",
    "\t\tself.construct_input()\n",
    "\n",
    "\tdef construct_input(self):\n",
    "\t\tdef change_to_one_hot(value, value_range):\n",
    "\t\t\tone_hot_vec = np.zeros(len(value_range))\n",
    "\t\t\tone_hot_vec[int(value/.5)] = 1\n",
    "\t\t\treturn one_hot_vec\n",
    "\n",
    "\t\tm = self.m\n",
    "\t\tn = self.n\n",
    "\n",
    "\t\tuser_indices, movie_indices = (np.where(self.train_data > 0))\n",
    "\t\tscores = self.train_data[self.mask]\n",
    "\n",
    "\t\tnum_train_samples = user_indices.shape[0]\n",
    "\n",
    "\t\tself.train_x = np.zeros((num_train_samples, m+n))\n",
    "\t\tself.train_y = np.zeros((num_train_samples, 11))\n",
    "\n",
    "\t\tstart = time.time()\n",
    "\n",
    "\t\t#construct training input and output X, y\n",
    "\t\tfor i in range(num_train_samples):\n",
    "\t\t\tu_ind = user_indices[i]\n",
    "\t\t\tm_ind = movie_indices[i]\n",
    "\n",
    "\t\t\tself.train_x[i, u_ind]   = 1\n",
    "\t\t\tself.train_x[i, m+m_ind] = 1\n",
    "\n",
    "\t\t\tscore \t\t\t= self.train_data[u_ind, m_ind]\n",
    "\t\t\tself.train_y[i] = change_to_one_hot(score, np.arange(0,5.5,.5))\n",
    "\n",
    "\t\t#construct test inputs for where we need to predict values\n",
    "\t\tuser_indices, movie_indices = np.where(self.mask)\n",
    "\t\tnum_test_samples = user_indices.shape[0]\n",
    "\t\tself.test_x = np.zeros((num_test_samples, m+n))\n",
    "\t\tself.test_y = np.zeros((num_test_samples, 11))\n",
    "\n",
    "\t\tfor i in range(num_test_samples):\n",
    "\t\t\tu_ind = user_indices[i]\n",
    "\t\t\tm_ind = movie_indices[i]\n",
    "\n",
    "\t\t\tself.test_x[i, u_ind]   = 1\n",
    "\t\t\tself.test_x[i, m+m_ind] = 1\n",
    "\n",
    "\t\t\tscore \t\t   = self.val_data[u_ind, m_ind]\n",
    "\t\t\tself.test_y[i] = change_to_one_hot(score, np.arange(0,5.5,.5))\n",
    "\n",
    "\t\tprint(time.time() - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef construct_model(self, hidden_layer_pattern = 'exponential'):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinput_size = self.m + self.n\n",
    "\t\t\n",
    "\t\t# add the first layer\n",
    "\t\tmodel.add(Dense(input_size, activation='relu', input_shape=(input_size,)))\n",
    "\n",
    "\t\t#one of the two model architectures tested\n",
    "\t\tif (hidden_layer_pattern == 'linear'):\n",
    "\t\t\tlinear_decrease = int(input_size/self.num_layers)\n",
    "\t\t\tfor i in range(self.num_layers):\n",
    "\t\t\t\tinput_size = input_size - linear_decrease\n",
    "\t\t\t\tmodel.add(Dense(input_size, activation='relu') )\n",
    "\n",
    "\t\tif (hidden_layer_pattern == 'exponential'):\n",
    "\t\t\texponential_decrease = int((np.exp(np.log(input_size)/(self.num_layers+2))))\n",
    "\t\t\tprint(exponential_decrease)\n",
    "\t\t\tfor i in range(self.num_layers):\n",
    "\t\t\t\tinput_size = int(input_size/exponential_decrease);\n",
    "\t\t\t\tmodel.add(Dense(input_size, activation='relu') )\n",
    "\n",
    "\t\tprint (model.output_shape)\n",
    "        \n",
    "\t\t#one hot encoded output\n",
    "\t\tmodel.add(Dense(11, activation='relu'))\n",
    "\n",
    "\t\tadam = optimizers.Adam(lr=self.learn_rate, decay=.001)\n",
    "\t\tmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\t\tself.model = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and train Model\n",
    "\n",
    "Trains the model. Saves checkpoints of the model at every epoch.\n",
    "Param:\n",
    "    model_number - Just changes the filename that the model is saved to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(self, model_number = 0):\n",
    "\n",
    "\t\t# lets make checkpoints\n",
    "\t\tfilepath = \"nn_model_{}_lr_{}\".format(model_number, self.learn_rate)\n",
    "\t\tfilepath+= \"_{epoch:02d}.hdf5\"\n",
    "\n",
    "\t\tprint('learn_rate = {}'.format(self.learn_rate))\n",
    "\t\tcheckpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\t\tcallbacks_list = [checkpoint]\n",
    "\n",
    "        #***CHANGE EPOCH NUMBER HERE\n",
    "\t\tself.model.fit(self.train_x, self.train_y, batch_size=128, epochs=20, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\tdef load_model(self, filename):\n",
    "\t\t'''\n",
    "\t\tLoads the weights of an identically architectured neural net at the given filepath\n",
    "\t\t'''\n",
    "\t\tself.model.load_weights(filename)\n",
    "\t\tadam = optimizers.Adam(lr=self.learn_rate, decay=.001)\n",
    "\t\tself.model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT\n",
    "Predicts values based on training or validation data\n",
    "Return:\n",
    "    scores\n",
    "    predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_values(self, test_type='validation'):\n",
    "    # print(self.model.get_weights())\n",
    "    if (test_type == 'validation'):\n",
    "        scores = self.model.predict(self.test_x, verbose=True)\n",
    "        return scores, self.test_y\n",
    "    elif (test_type == 'training'):\n",
    "        scores = self.model.predict(self.train_x, verbose=True)\n",
    "        return scores, self.train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN AND TEST\n",
    "\n",
    "Trains a neural net and saves snapshots every epoch. Runs 20 epochs or until you quit the process. Gets the accuracy and validation error of a model. This function assumes you have been saving your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import util\n",
    "from nn import Collaborative_Filtering_Neural_Net\n",
    "\n",
    "def train():\n",
    "\ttrain_mat, val_mat, masks = util.k_cross()\n",
    "\tA = util.load_data_matrix()\n",
    "\n",
    "\tstart = time.time()\n",
    "\tnet = Collaborative_Filtering_Neural_Net(train_mat[0], val_mat[0], masks[0])\n",
    "\tnet.learn_rate=.1\n",
    "\tnet.construct_model(hidden_layer_pattern = 'exponential')\n",
    "\t# net.load_model('nn_model_exponential_one_hot_learn_rate_.1_lr_0.1_04.hdf5')\n",
    "\tnet.train_model(model_number='exponential_one_hot')\n",
    "\tprint('time taken to train in seconds:', time.time() - start)\n",
    "\n",
    "def test(model_name = '', test_type = 'validation'):\n",
    "\ttrain_mat, val_mat, masks = util.k_cross()\n",
    "\tA = util.load_data_matrix()\n",
    "\n",
    "\tnet = Collaborative_Filtering_Neural_Net(train_mat[0], val_mat[0], masks[0])\n",
    "\tnet.learn_rate=.1\n",
    "\tnet.construct_model(hidden_layer_pattern = 'exponential')\n",
    "\tnet.load_model(model_name)\n",
    "\n",
    "\tpred_scores , true_scores= net.predict_values(test_type = test_type)\n",
    "\tpred_scores = pred_scores.argmax(axis=1)\n",
    "\ttrue_scores    = true_scores.argmax(axis=1)\n",
    "\n",
    "\t#get Accuracy\n",
    "\tnum_correct = np.sum(pred_scores == true_scores)\n",
    "\taccuracy    = num_correct/pred_scores.shape[0]*100\n",
    "\n",
    "\t#get MSE\n",
    "\terror = pred_scores-true_scores\n",
    "\tmse   = np.mean(np.power(error, 2))\n",
    "\n",
    "\tprint('The {} accuracy of the model is {}%'.format(test_type, accuracy))\n",
    "\tprint('The {} mean squared error of the model is {}'.format(test_type, mse))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttrain()\n",
    "\ttest('nn_model_exponential_one_hot_round_2_lr_0.1_08.hdf5', test_type='training')\n",
    "\ttest('nn_model_exponential_one_hot_round_2_lr_0.1_08.hdf5', test_type='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility\n",
    "\n",
    "Create the training and validation matrices for k_cross validaton\n",
    "\tparam:\n",
    "\t\tfilename - name of pickle file with original (m x n) data matrix, must have file extension\n",
    "\t\tpath     - path to the filename - can be omitted if path is included in filename\n",
    "\t\tk        - number of training/data sets\n",
    "\treturns:\n",
    "\t\ttraining_matrices   - array of (m x n) training matrices\n",
    "\t\tprediction_matrices - array of (m x n) validation matrices where each data point is omitted from the corresponding \n",
    "\t\t\t\t\t\t\t  training matrix\n",
    "\t\tindex_matrices      - array of (m x n) boolean masks where each true element is where an element was transplanted\n",
    "\t\t\t\t\t\t\t  from the training matrix to the validation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "def k_cross(k = 10):\n",
    "\n",
    "\tA = load_data_matrix()\n",
    "\tm = A.shape[0]\n",
    "\tn = A.shape[1]\n",
    "\n",
    "\tprint('A.shape = {}'.format(A.shape))\n",
    "\n",
    "\tprediction_matrices = []\n",
    "\ttraining_matrices   = []\n",
    "\tindex_lists         = []\n",
    "\tfor i in range(k):\n",
    "\t    A_copy = A.copy()\n",
    "\t    prediction_matrices.append(np.zeros((m, n)))\n",
    "\t    training_matrices.append(A_copy)\n",
    "\t    index_lists.append(np.zeros((m, n), dtype=bool))\n",
    "\n",
    "\tit    = 0\n",
    "\tfor i in range(A.shape[0]):\n",
    "\t    for j in range(A.shape[1]):\n",
    "\t        if (A[i, j] != 0):\n",
    "\t            training_matrices[it%k][i, j]   = 0\n",
    "\t            prediction_matrices[it%k][i, j] = A[i, j]\n",
    "\t            index_lists[it%k][i, j] = True\n",
    "\t            it+=1\n",
    "\n",
    "\treturn training_matrices, prediction_matrices, index_lists\n",
    "\n",
    "def load_data_matrix(filename='data_matrix.p', path='data'):\n",
    "\tfilepath = filename if path == '' else '{}/{}'.format(path,filename)\n",
    "\tA = pickle.load( open('{}'.format(filepath), 'rb'))\n",
    "\treturn A\n",
    "\n",
    "def get_MSE(mat1, mask, mat2=''):\n",
    "\tif (mat2 == ''):\n",
    "\t\tmat2 = load_data_matrix()\n",
    "\n",
    "\tA_mask    = mat2[mask]\n",
    "\tmat1_mask = mat1[mask]\n",
    "\n",
    "\tdiff = A_mask-mat1_mask\n",
    "\tmse = np.dot(diff, diff)/A_mask.shape\n",
    "\treturn mse[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tk = 10\n",
    "\n",
    "\ttrain_mats, val_mats, masks = k_cross(k=k)\n",
    "\tprint('MSE = {}'.format(get_MSE(train_mats[0], masks[0])))\n",
    "\n",
    "\tm = train_mats[0].shape[0]\n",
    "\tn = train_mats[0].shape[1]\n",
    "\tstart = time.time()\n",
    "\tfor i in range(m):\n",
    "\t    for j in range(n):\n",
    "\t        for index in range(k):\n",
    "\t            if(train_mats[index][i,j] != 0 and val_mats[index][i,j] != 0):\n",
    "\t                print('we have a problem')\n",
    "\tend = time.time()\n",
    "\tprint('you wasted {} seconds of my life'.format(end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
