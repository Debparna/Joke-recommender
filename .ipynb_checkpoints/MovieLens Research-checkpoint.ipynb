{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation \n",
    "\n",
    "\n",
    "## The MovieLens Dataset\n",
    "Contains 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6040, 5) 6040\n",
      "(3883, 3) 3883\n",
      "(1000209, 4) 1000209\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading ratings file\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating','user_emb_id'])\n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "print(users.shape, len(users))\n",
    "print(movies.shape, len(movies))\n",
    "print(ratings.shape, len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id  rating  user_emb_id\n",
      "0        1      1193       5            0\n",
      "1        1       661       3            0\n",
      "2        1       914       3            0\n",
      "3        1      3408       4            0\n",
      "4        1      2355       5            0\n"
     ]
    }
   ],
   "source": [
    "# Check the top 5 rows\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000209 entries, 0 to 1000208\n",
      "Data columns (total 4 columns):\n",
      "user_id        1000209 non-null int64\n",
      "movie_id       1000209 non-null int64\n",
      "rating         1000209 non-null int64\n",
      "user_emb_id    1000209 non-null int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 30.5 MB\n",
      "None\n",
      "Describe Ratings: \n",
      " count    1.000209e+06\n",
      "mean     3.581564e+00\n",
      "std      1.117102e+00\n",
      "min      1.000000e+00\n",
      "25%      3.000000e+00\n",
      "50%      4.000000e+00\n",
      "75%      4.000000e+00\n",
      "max      5.000000e+00\n",
      "Name: rating, dtype: float64\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f0c41a84d090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Import seaborn library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'whitegrid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Check the file info\n",
    "print(ratings.info())\n",
    "# Get summary statistics of rating\n",
    "print(\"Describe Ratings: \\n\", ratings['rating'].describe())\n",
    "\n",
    "\n",
    "# Import seaborn library\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(font_scale=1.5)\n",
    "%matplotlib inline\n",
    "\n",
    "# Display distribution of rating\n",
    "sns.distplot(ratings['rating'].fillna(ratings['rating'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id gender zipcode  age_desc              occ_desc\n",
      "0        1      F   48067  Under 18          K-12 student\n",
      "1        2      M   70072       56+         self-employed\n",
      "2        3      M   55117     25-34             scientist\n",
      "3        4      M   02460     45-49  executive/managerial\n",
      "4        5      M   55455     25-34                writer\n"
     ]
    }
   ],
   "source": [
    "# Check the top 5 rows\n",
    "print(users.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6040 entries, 0 to 6039\n",
      "Data columns (total 5 columns):\n",
      "user_id     6040 non-null int64\n",
      "gender      6040 non-null object\n",
      "zipcode     6040 non-null object\n",
      "age_desc    6040 non-null object\n",
      "occ_desc    6040 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 236.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check the file info\n",
    "print(users.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id                               title                        genres\n",
      "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4         5  Father of the Bride Part II (1995)                        Comedy\n"
     ]
    }
   ],
   "source": [
    "# Check the top 5 rows\n",
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3883 entries, 0 to 3882\n",
      "Data columns (total 3 columns):\n",
      "movie_id    3883 non-null int64\n",
      "title       3883 non-null object\n",
      "genres      3883 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 91.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check the file info\n",
    "print(movies.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Drama', 1603],\n",
       " ['Comedy', 1200],\n",
       " ['Action', 503],\n",
       " ['Thriller', 492],\n",
       " ['Romance', 471]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading ratings file\n",
    "# Ignore the timestamp column\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "# Join all 3 files into one dataframe\n",
    "dataset = pd.merge(pd.merge(movies, ratings),users)\n",
    "\n",
    "# Display 20 movies with highest ratings\n",
    "dataset[['title','genres','rating']].sort_values('rating', ascending=False).head(20)\n",
    "\n",
    "# Make a census of the genre keywords\n",
    "genre_labels = set()\n",
    "for s in movies['genres'].str.split('|').values:\n",
    "    genre_labels = genre_labels.union(set(s))\n",
    "\n",
    "# Function that counts the number of times each of the genre keywords appear\n",
    "def count_word(dataset, ref_col, census):\n",
    "    keyword_count = dict()\n",
    "    for s in census: \n",
    "        keyword_count[s] = 0\n",
    "    for census_keywords in dataset[ref_col].str.split('|'):        \n",
    "        if type(census_keywords) == float and pd.isnull(census_keywords): \n",
    "            continue        \n",
    "        for s in [s for s in census_keywords if s in census]: \n",
    "            if pd.notnull(s): \n",
    "                keyword_count[s] += 1\n",
    "    #______________________________________________________________________\n",
    "    # convert the dictionary in a list to sort the keywords by frequency\n",
    "    keyword_occurences = []\n",
    "    for k,v in keyword_count.items():\n",
    "        keyword_occurences.append([k,v])\n",
    "    keyword_occurences.sort(key = lambda x:x[1], reverse = True)\n",
    "    return keyword_occurences, keyword_count\n",
    "\n",
    "# Calling this function gives access to a list of genre keywords which are sorted by decreasing frequency\n",
    "keyword_occurences, dum = count_word(movies, 'genres', genre_labels)\n",
    "keyword_occurences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#****Changing Dataset size\n",
    "\n",
    "percent  = 0.2 #0.1 #0.3\n",
    "chosen_idx = np.random.choice(len(users), replace=False, size=int(len(users)*percent))\n",
    "users = users.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(movies), replace=False, size=int(len(movies)*percent))\n",
    "movies = movies.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(ratings), replace=False, size=int(len(ratings)*percent))\n",
    "ratings = ratings.iloc[chosen_idx]\n",
    "\n",
    "\n",
    "#print(user_20.shape, movie_20.shape, ratings_20.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based filtering\n",
    "A Content-Based Recommendation Engine that computes similarity between movies based on movie genres. It will suggest movies that are most similar to a particular movie based on its genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Break up the big genre string into a string array\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "# Convert genres to string value\n",
    "movies['genres'] = movies['genres'].fillna(\"\").astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To judge the machine's performance qualitatively use TfidfVectorizer function from scikit-learn, which transforms text to feature vectors that can be used as input to estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(movies['genres'])\n",
    "tfidf_matrix.shape\n",
    "\n",
    "#Cosine Similarity to calculate a numeric quantity that denotes the similarity between two movies.\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "cosine_sim[:4, :4]\n",
    "\n",
    "# Build a 1-dimensional array with movie titles\n",
    "titles = movies['title']\n",
    "indices = pd.Series(movies.index, index=movies['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that gets movie recommendations based on the cosine similarity score of movie genres. It returns the 20 most \n",
    "similar movies based on the cosine similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_recommendations(title):\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:21]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return titles.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top recommendations for a few movies\n",
    "#genre_recommendations('Good Will Hunting (1997)').head(20)\n",
    "#genre_recommendations('Saving Private Ryan (1998)').head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Fill NaN values in user_id and movie_id column with 0\n",
    "ratings['user_id'] = ratings['user_id'].fillna(0)\n",
    "ratings['movie_id'] = ratings['movie_id'].fillna(0)\n",
    "\n",
    "# Replace NaN values in rating column with average of all values\n",
    "ratings['rating'] = ratings['rating'].fillna(ratings['rating'].mean())\n",
    "\n",
    "#***CHANGING DATASET SIZE for experimentation\n",
    "# Randomly sample 2% of the ratings dataset\n",
    "small_data = ratings.sample(frac=0.02)\n",
    "\n",
    "train_data, test_data = cv.train_test_split(small_data, test_size=0.2)\n",
    "\n",
    "# Create two user-item matrices, one for training and another for testing\n",
    "train_data_matrix = train_data.as_matrix(columns = ['user_id', 'movie_id', 'rating'])\n",
    "test_data_matrix = test_data.as_matrix(columns = ['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "# Check their shape\n",
    "#print(train_data_matrix.shape)\n",
    "#print(test_data_matrix.shape)\n",
    "\n",
    "# User Similarity Matrix\n",
    "user_correlation = 1 - pairwise_distances(train_data, metric='correlation')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "#print(user_correlation[:4, :4])\n",
    "\n",
    "# Item Similarity Matrix\n",
    "item_correlation = 1 - pairwise_distances(train_data_matrix.T, metric='correlation')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "#print(item_correlation[:4, :4])\n",
    "\n",
    "# Function to predict ratings\n",
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        # Use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models\n",
    "\n",
    "The basic idea is that the actual ratings of movies for each user can be represented by a matrix, say of users on the rows and movies along the columns.  \n",
    "We don’t have the full rating matrix, instead, we have a very sparse set of entries. \n",
    "But if we could factor the rating matrix into two separate matrices, say one that was Used by Latent Factors, and one that was Latent Factors by Movies, then we could find the user’s rating for any movie by taking the dot product of the User row and the Movie column.\n",
    "\n",
    "Y = Ratings.\n",
    "X1, X2 = Movies, Users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model 1\n",
    "\n",
    "The user latent features and movie latent features are looked up from the embedding matrices for specific movie-user combination. These are the input values for further linear and non-linear layers. We can pass this input to multiple relu, linear or sigmoid layers and learn the corresponding weights by any optimization algorithm (Adam, SGD, etc.).\n",
    "\n",
    "## The Model\n",
    "- A left embedding layer that creates a Users by Latent Factors matrix.\n",
    "- A right embedding layer that creates a Movies by Latent Factors matrix.\n",
    "- When the input to these layers are (i) a user id and (ii) a movie id, they'll return the latent factor vectors for the user and the movie, respectively.\n",
    "A merge layer that takes the dot product of these two latent vectors to return the predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Merge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-88b6fb0e15a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCFModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Merge'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Embedding, Reshape, Merge\n",
    "from keras.models import Sequential\n",
    "\n",
    "class CFModel(Sequential):\n",
    "\n",
    "    # The constructor for the class\n",
    "    def __init__(self, n_users, m_items, k_factors, **kwargs):\n",
    "        # P is the embedding layer that creates an User by latent factors matrix.\n",
    "        # If the intput is a user_id, P returns the latent factor vector for that user.\n",
    "        P = Sequential()\n",
    "        P.add(Embedding(n_users, k_factors, input_length=1))\n",
    "        P.add(Reshape((k_factors,)))\n",
    "\n",
    "        # Q is the embedding layer that creates a Movie by latent factors matrix.\n",
    "        # If the input is a movie_id, Q returns the latent factor vector for that movie.\n",
    "        Q = Sequential()\n",
    "        Q.add(Embedding(m_items, k_factors, input_length=1))\n",
    "        Q.add(Reshape((k_factors,)))\n",
    "\n",
    "        super(CFModel, self).__init__(**kwargs)\n",
    "        \n",
    "        # The Merge layer takes the dot product of user and movie latent factor vectors to return the corresponding rating.\n",
    "        self.add(Merge([P, Q], mode='dot', dot_axes=1))\n",
    "\n",
    "    # The rate function to predict user's rating of unrated items\n",
    "    def rate(self, user_id, item_id):\n",
    "        return self.predict([np.array([user_id]), np.array([item_id])])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: [4374  838 3588 ... 5819  550 4657] , shape = (200041,)\n",
      "Movies: [ 439 3407 2744 ... 1264  456  312] , shape = (200041,)\n",
      "Ratings: [3 4 5 ... 2 4 2] , shape = (200041,)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "#Loading the datasets again\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'user_emb_id', 'movie_emb_id', 'rating'])\n",
    "max_userid = ratings['user_id'].drop_duplicates().max()\n",
    "max_movieid = ratings['movie_id'].drop_duplicates().max()\n",
    "\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "#Taking 20% of the dataset for epoch experimentation\n",
    "percent  = 0.2\n",
    "chosen_idx = np.random.choice(len(users), replace=False, size=int(len(users)*percent))\n",
    "users = users.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(movies), replace=False, size=int(len(movies)*percent))\n",
    "movies = movies.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(ratings), replace=False, size=int(len(ratings)*percent))\n",
    "ratings = ratings.iloc[chosen_idx]\n",
    "\n",
    "#print(user_20.shape, movie_20.shape, ratings_20.shape)\n",
    "\n",
    "# Create training set\n",
    "shuffled_ratings = ratings.sample(frac=1.)\n",
    "\n",
    "# Shuffling users\n",
    "Users = shuffled_ratings['user_emb_id'].values\n",
    "print('Users:', Users, ', shape =', Users.shape)\n",
    "# Shuffling movies\n",
    "Movies = shuffled_ratings['movie_emb_id'].values\n",
    "print('Movies:', Movies, ', shape =', Movies.shape)\n",
    "# Shuffling ratings\n",
    "Ratings = shuffled_ratings['rating'].values\n",
    "print('Ratings:', Ratings, ', shape =', Ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CFModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-9e5f8b5d8e17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCFModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_userid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_movieid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_FACTORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Compile the model using MSE as the loss function and the AdaMax learning algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adamax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CFModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "K_FACTORS = 100 # The number of dimensional embeddings for movies and users\n",
    "TEST_USER = 2000 # A random test user (user_id = 2000)\n",
    "\n",
    "# Define model\n",
    "model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "# Compile the model using MSE as the loss function and the AdaMax learning algorithm\n",
    "model.compile(loss='mse', optimizer='adamax')\n",
    "\n",
    "# Callbacks monitor the validation loss\n",
    "# Save the model weights each time the validation loss has improved\n",
    "callbacks = [EarlyStopping('val_loss', patience=2), \n",
    "             ModelCheckpoint('weights.h5', save_best_only=True)]\n",
    "\n",
    "# Use 30 epochs, 90% training data, 10% validation data \n",
    "history = model.fit([Users, Movies], Ratings, nb_epoch=30, validation_split=.1, verbose=2, callbacks=callbacks)\n",
    "\n",
    "###### RMSE\n",
    "# Show the best validation RMSE\n",
    "min_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history.history['val_loss']))\n",
    "print('Minimum RMSE at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(math.sqrt(min_val_loss)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CFModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-83829c10e38f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use the pre-trained model, load pre-trained weights from weights.h5 for the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCFModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_userid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_movieid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_FACTORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CFModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the pre-trained model, load pre-trained weights from weights.h5 for the model.\n",
    "trained_model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "# Load weights\n",
    "trained_model.load_weights('weights.h5')\n",
    "\n",
    "# Pick a random test user, random test user is has ID 2000.\n",
    "users[users['user_id'] == TEST_USER]\n",
    "\n",
    "# Function to predict the ratings given User ID and Movie ID\n",
    "def predict_rating(user_id, movie_id):\n",
    "    return trained_model.rate(user_id - 1, movie_id - 1)\n",
    "\n",
    "user_ratings = ratings[ratings['user_id'] == TEST_USER][['user_id', 'movie_id', 'rating']]\n",
    "user_ratings['prediction'] = user_ratings.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
    "user_ratings.sort_values(by='rating', \n",
    "                         ascending=False).merge(movies, \n",
    "                                                on='movie_id', \n",
    "                                                how='inner', \n",
    "                                                suffixes=['_u', '_m']).head(20)\n",
    "\n",
    "recommendations = ratings[ratings['movie_id'].isin(user_ratings['movie_id']) == False][['movie_id']].drop_duplicates()\n",
    "recommendations['prediction'] = recommendations.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
    "recommendations.sort_values(by='prediction',\n",
    "                          ascending=False).merge(movies,\n",
    "                                                 on='movie_id',\n",
    "                                                 how='inner',\n",
    "                                                 suffixes=['_u', '_m']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model 2\n",
    "Here is the model function but the implementation of the model is in file experimentalDNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
