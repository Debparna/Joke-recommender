{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation \n",
    "\n",
    "\n",
    "## The MovieLens Dataset\n",
    "Contains 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6040, 5) 6040\n",
      "(3883, 3) 3883\n",
      "(1000209, 4) 1000209\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating','user_emb_id'])\n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "\n",
    "print(users.shape, len(users))\n",
    "print(movies.shape, len(movies))\n",
    "print(ratings.shape, len(ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading ratings file\n",
    "# Ignore the timestamp column\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10002, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "percent  = 0.01\n",
    "chosen_idx = np.random.choice(len(users), replace=False, size=int(len(users)*percent))\n",
    "users = users.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(movies), replace=False, size=int(len(movies)*percent))\n",
    "movies = movies.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(ratings), replace=False, size=int(len(ratings)*percent))\n",
    "ratings = ratings.iloc[chosen_idx]\n",
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two collaborative filtering approaches: Memory-based approach and Model-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/2000/1*7uW5hLXztSu_FOmZOWpB6g.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://cdn-images-1.medium.com/max/2000/1*7uW5hLXztSu_FOmZOWpB6g.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Memory based approach: \n",
    "### User-based filtering and Item-based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/PY36/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2001, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Fill NaN values in user_id and movie_id column with 0\n",
    "ratings['user_id'] = ratings['user_id'].fillna(0)\n",
    "ratings['movie_id'] = ratings['movie_id'].fillna(0)\n",
    "\n",
    "# Replace NaN values in rating column with average of all values\n",
    "ratings['rating'] = ratings['rating'].fillna(ratings['rating'].mean())\n",
    "\n",
    "train_data, test_data = cv.train_test_split(ratings, test_size=0.2)\n",
    "\n",
    "# Create two user-item matrices, one for training and another for testing\n",
    "train_data_matrix = train_data.as_matrix(columns = ['rating'])\n",
    "test_data_matrix = test_data.as_matrix(columns = ['rating'])\n",
    "\n",
    "# Check their shape\n",
    "\n",
    "print(test_data_matrix.shape)\n",
    "\n",
    "# User Similarity Matrix\n",
    "user_correlation = 1 - pairwise_distances(train_data, metric='cosine')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "#print(user_correlation[:4, :4])\n",
    "\n",
    "# Item Similarity Matrix\n",
    "item_correlation = 1 - pairwise_distances(train_data_matrix.T, metric='cosine')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "#print(item_correlation[:4, :4])\n",
    "\n",
    "# Function to predict ratings\n",
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        # Use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    pred = np.array(pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 1.5748840814691625\n",
      "Item-based CF RMSE: 1.5748840814691625\n",
      "User-based CF RMSE: 0.0\n",
      "Item-based CF RMSE: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(pred, actual))\n",
    "# Predict ratings on the training data with both similarity score\n",
    "user_prediction = predict(train_data_matrix, user_correlation, type='user')\n",
    "item_prediction = predict(train_data_matrix, item_correlation, type='item')\n",
    "\n",
    "# RMSE on the test data\n",
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix)))\n",
    "# RMSE on the train data\n",
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, train_data_matrix)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, train_data_matrix)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Deep Learning Models\n",
    "\n",
    "The basic idea is that the actual ratings of movies for each user can be represented by a matrix, say of users on the rows and movies along the columns.  \n",
    "We don’t have the full rating matrix, instead, we have a very sparse set of entries. \n",
    "But if we could factor the rating matrix into two separate matrices, say one that was Used by Latent Factors, and one that was Latent Factors by Movies, then we could find the user’s rating for any movie by taking the dot product of the User row and the Movie column.\n",
    "\n",
    "Y = Ratings.\n",
    "X1, X2 = Movies, Users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Embedding, Reshape, Merge\n",
    "from keras.models import Sequential\n",
    "\n",
    "class CFModel(Sequential):\n",
    "\n",
    "    # The constructor for the class\n",
    "    def __init__(self, n_users, m_items, k_factors, **kwargs):\n",
    "        # P is the embedding layer that creates an User by latent factors matrix.\n",
    "        # If the intput is a user_id, P returns the latent factor vector for that user.\n",
    "        P = Sequential()\n",
    "        P.add(Embedding(n_users, k_factors, input_length=1))\n",
    "        P.add(Reshape((k_factors,)))\n",
    "\n",
    "        # Q is the embedding layer that creates a Movie by latent factors matrix.\n",
    "        # If the input is a movie_id, Q returns the latent factor vector for that movie.\n",
    "        Q = Sequential()\n",
    "        Q.add(Embedding(m_items, k_factors, input_length=1))\n",
    "        Q.add(Reshape((k_factors,)))\n",
    "\n",
    "        super(CFModel, self).__init__(**kwargs)\n",
    "        \n",
    "        # The Merge layer takes the dot product of user and movie latent factor vectors to return the corresponding rating.\n",
    "        self.add(Merge([P, Q], mode='dot', dot_axes=1))\n",
    "\n",
    "    # The rate function to predict user's rating of unrated items\n",
    "    def rate(self, user_id, item_id):\n",
    "        return self.predict([np.array([user_id]), np.array([item_id])])[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1208, 5) (776, 3) (200041, 3)\n",
      "Users: [5503 5687 3945 ... 4169  215  756] , shape = (200041,)\n",
      "Movies: [2825 1672 3173 ... 1372 3143  456] , shape = (200041,)\n",
      "Ratings: [4 1 3 ... 3 3 5] , shape = (200041,)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'user_emb_id', 'movie_emb_id', 'rating'])\n",
    "max_userid = ratings['user_id'].drop_duplicates().max()\n",
    "max_movieid = ratings['movie_id'].drop_duplicates().max()\n",
    "\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "import numpy as np\n",
    "percent  = 0.2\n",
    "chosen_idx = np.random.choice(len(users), replace=False, size=int(len(users)*percent))\n",
    "users = users.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(movies), replace=False, size=int(len(movies)*percent))\n",
    "movies = movies.iloc[chosen_idx]\n",
    "chosen_idx = np.random.choice(len(ratings), replace=False, size=int(len(ratings)*percent))\n",
    "ratings = ratings.iloc[chosen_idx]\n",
    "\n",
    "\n",
    "print(user_20.shape, movie_20.shape, ratings_20.shape)\n",
    "# Create training set\n",
    "shuffled_ratings = ratings.sample(frac=1.)\n",
    "\n",
    "\n",
    "# Shuffling users\n",
    "Users = shuffled_ratings['user_emb_id'].values\n",
    "print('Users:', Users, ', shape =', Users.shape)\n",
    "# Shuffling movies\n",
    "Movies = shuffled_ratings['movie_emb_id'].values\n",
    "print('Movies:', Movies, ', shape =', Movies.shape)\n",
    "\n",
    "# Shuffling ratings\n",
    "Ratings = shuffled_ratings['rating'].values\n",
    "print('Ratings:', Ratings, ', shape =', Ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/PY36/lib/python3.6/site-packages/ipykernel/__main__.py:24: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/anaconda3/envs/PY36/lib/python3.6/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180036 samples, validate on 20005 samples\n",
      "Epoch 1/30\n",
      " - 56s - loss: 14.0542 - val_loss: 13.8855\n",
      "Epoch 2/30\n",
      " - 55s - loss: 12.6226 - val_loss: 10.3997\n",
      "Epoch 3/30\n",
      " - 59s - loss: 7.3539 - val_loss: 5.3291\n",
      "Epoch 4/30\n",
      " - 52s - loss: 4.0102 - val_loss: 3.3089\n",
      "Epoch 5/30\n",
      " - 50s - loss: 2.6277 - val_loss: 2.3712\n",
      "Epoch 6/30\n",
      " - 49s - loss: 1.9542 - val_loss: 1.8864\n",
      "Epoch 7/30\n",
      " - 53s - loss: 1.5840 - val_loss: 1.6042\n",
      "Epoch 8/30\n",
      " - 54s - loss: 1.3594 - val_loss: 1.4296\n",
      "Epoch 9/30\n",
      " - 54s - loss: 1.2112 - val_loss: 1.3092\n",
      "Epoch 10/30\n"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "K_FACTORS = 100 # The number of dimensional embeddings for movies and users\n",
    "TEST_USER = 2000 # A random test user (user_id = 2000)\n",
    "\n",
    "# Define model\n",
    "model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "# Compile the model using MSE as the loss function and the AdaMax learning algorithm\n",
    "model.compile(loss='mse', optimizer='adamax')\n",
    "\n",
    "# Callbacks monitor the validation loss\n",
    "# Save the model weights each time the validation loss has improved\n",
    "callbacks = [EarlyStopping('val_loss', patience=2), \n",
    "             ModelCheckpoint('weights.h5', save_best_only=True)]\n",
    "\n",
    "# Use 30 epochs, 90% training data, 10% validation data \n",
    "history = model.fit([Users, Movies], Ratings, nb_epoch=30, validation_split=.1, verbose=2, callbacks=callbacks)\n",
    "\n",
    "# Show the best validation RMSE\n",
    "min_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history.history['val_loss']))\n",
    "print('Minimum RMSE at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(math.sqrt(min_val_loss)))\n",
    "\n",
    "# Use the pre-trained model\n",
    "trained_model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "# Load weights\n",
    "trained_model.load_weights('weights.h5')\n",
    "\n",
    "# Pick a random test user\n",
    "users[users['user_id'] == TEST_USER]\n",
    "\n",
    "# Function to predict the ratings given User ID and Movie ID\n",
    "def predict_rating(user_id, movie_id):\n",
    "    return trained_model.rate(user_id - 1, movie_id - 1)\n",
    "\n",
    "user_ratings = ratings[ratings['user_id'] == TEST_USER][['user_id', 'movie_id', 'rating']]\n",
    "user_ratings['prediction'] = user_ratings.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
    "user_ratings.sort_values(by='rating', \n",
    "                         ascending=False).merge(movies, \n",
    "                                                on='movie_id', \n",
    "                                                how='inner', \n",
    "                                                suffixes=['_u', '_m']).head(20)\n",
    "\n",
    "recommendations = ratings[ratings['movie_id'].isin(user_ratings['movie_id']) == False][['movie_id']].drop_duplicates()\n",
    "recommendations['prediction'] = recommendations.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
    "recommendations.sort_values(by='prediction',\n",
    "                          ascending=False).merge(movies,\n",
    "                                                 on='movie_id',\n",
    "                                                 how='inner',\n",
    "                                                 suffixes=['_u', '_m']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class Collaborative_Filtering_Neural_Net(object):\n",
    "\n",
    "\tdef __init__(self, train_data, val_data, mask, num_layers=3, learn_rate=.2):\n",
    "\n",
    "\t\tself.train_data = train_data\n",
    "\t\tself.val_data   = val_data\n",
    "\t\tself.mask       = mask\n",
    "\t\tself.num_layers = num_layers\n",
    "\n",
    "\t\tself.m          = self.train_data.shape[0]\n",
    "\t\tself.n \t\t\t= self.train_data.shape[1]\n",
    "\t\t\n",
    "\t\tself.learn_rate = learn_rate\n",
    "\n",
    "\t\tself.construct_input()\n",
    "\n",
    "\n",
    "\tdef construct_input(self):\n",
    "\t\t'''\n",
    "\t\tConstruct training input/output from the training data matrix\n",
    "\t\tand \n",
    "\t\tConstruct validation input/output from the training/validation \n",
    "\t\t'''\n",
    "\t\tdef change_to_one_hot(value, value_range):\n",
    "\t\t\tone_hot_vec = np.zeros(len(value_range))\n",
    "\t\t\tone_hot_vec[int(value/.5)] = 1\n",
    "\t\t\treturn one_hot_vec\n",
    "\n",
    "\n",
    "\t\tm = self.m\n",
    "\t\tn = self.n\n",
    "\n",
    "\t\tuser_indices, movie_indices = (np.where(self.train_data > 0))\n",
    "\t\tscores = self.train_data[self.mask]\n",
    "\n",
    "\t\tnum_train_samples = user_indices.shape[0]\n",
    "\n",
    "\t\tself.train_x = np.zeros((num_train_samples, m+n))\n",
    "\t\tself.train_y = np.zeros((num_train_samples, 11))\n",
    "\n",
    "\t\tstart = time.time()\n",
    "\n",
    "\t\t#construct training input and output X, y\n",
    "\t\tfor i in range(num_train_samples):\n",
    "\t\t\tu_ind = user_indices[i]\n",
    "\t\t\tm_ind = movie_indices[i]\n",
    "\n",
    "\t\t\tself.train_x[i, u_ind]   = 1\n",
    "\t\t\tself.train_x[i, m+m_ind] = 1\n",
    "\n",
    "\t\t\tscore \t\t\t= self.train_data[u_ind, m_ind]\n",
    "\t\t\tself.train_y[i] = change_to_one_hot(score, np.arange(0,5.5,.5))\n",
    "\n",
    "\n",
    "\n",
    "\t\t#construct test inputs for where we need to predict values\n",
    "\t\tuser_indices, movie_indices = np.where(self.mask)\n",
    "\t\tnum_test_samples = user_indices.shape[0]\n",
    "\t\tself.test_x = np.zeros((num_test_samples, m+n))\n",
    "\t\tself.test_y = np.zeros((num_test_samples, 11))\n",
    "\n",
    "\t\tfor i in range(num_test_samples):\n",
    "\t\t\tu_ind = user_indices[i]\n",
    "\t\t\tm_ind = movie_indices[i]\n",
    "\n",
    "\t\t\tself.test_x[i, u_ind]   = 1\n",
    "\t\t\tself.test_x[i, m+m_ind] = 1\n",
    "\n",
    "\t\t\tscore \t\t   = self.val_data[u_ind, m_ind]\n",
    "\t\t\tself.test_y[i] = change_to_one_hot(score, np.arange(0,5.5,.5))\n",
    "\n",
    "\t\tprint(time.time() - start)\n",
    "\n",
    "\n",
    "\tdef construct_model(self, hidden_layer_pattern = 'exponential'):\n",
    "\t\t'''\n",
    "\t\tConstructs a Neural network with a given pattern.\n",
    "\t\tThe pattern indicates how many neurons should exist at every layer.\n",
    "\t\tParam:\n",
    "\t\t\thidden_layer_pattern - The input layer and output layer are fixed, but the rate at which the layer sizes\n",
    "\t\t\tdecreases depends on the parameter, hidden_layer_pattern\n",
    "\t\t'''\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinput_size = self.m + self.n\n",
    "\t\t\n",
    "\t\t# add the first layer\n",
    "\t\tmodel.add(Dense(input_size, activation='relu', input_shape=(input_size,)))\n",
    "\n",
    "\t\t#one of the two model architectures tested\n",
    "\t\tif (hidden_layer_pattern == 'linear'):\n",
    "\t\t\tlinear_decrease = int(input_size/self.num_layers)\n",
    "\t\t\tfor i in range(self.num_layers):\n",
    "\t\t\t\tinput_size = input_size - linear_decrease\n",
    "\t\t\t\tmodel.add(Dense(input_size, activation='relu') )\n",
    "\n",
    "\t\tif (hidden_layer_pattern == 'exponential'):\n",
    "\t\t\texponential_decrease = int((np.exp(np.log(input_size)/(self.num_layers+2))))\n",
    "\t\t\tprint(exponential_decrease)\n",
    "\t\t\tfor i in range(self.num_layers):\n",
    "\t\t\t\tinput_size = int(input_size/exponential_decrease);\n",
    "\t\t\t\tmodel.add(Dense(input_size, activation='relu') )\n",
    "\n",
    "\t\tprint (model.output_shape)\n",
    "\t\t#one hot encoded output\n",
    "\t\tmodel.add(Dense(11, activation='relu'))\n",
    "\n",
    "\n",
    "\t\t# model says they optimized the log loss error\n",
    "\n",
    "\t\tadam = optimizers.Adam(lr=self.learn_rate, decay=.001)\n",
    "\t\tmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\t\tself.model = model\n",
    "\n",
    "\tdef train_model(self, model_number = 0):\n",
    "\t\t'''\n",
    "\t\tTrains the model. Saves checkpoints of the model at every epoch.\n",
    "\t\tI personally just stop training when I find that the loss function has barely changed. Since it takes\n",
    "\t\tso long to perform each epoch on my computer, I just keep running a 20 epoch train, stop it when I\n",
    "\t\thave to, then train again later.\n",
    "\t\tParam:\n",
    "\t\t\tmodel_number - Just changes the filename that the model is saved to. \n",
    "\t\t\t\t\t\t   Don't want to overwrite good save files during training, do you?\n",
    "\n",
    "\t\tNote: these checkpoints are 1GB each.\n",
    "\t\t'''\n",
    "\t\t# lets make checkpoints\n",
    "\t\tfilepath = \"nn_model_{}_lr_{}\".format(model_number,self.learn_rate)\n",
    "\t\tfilepath+= \"_{epoch:02d}.hdf5\"\n",
    "\n",
    "\t\tprint('learn_rate = {}'.format(self.learn_rate))\n",
    "\t\tcheckpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\t\tcallbacks_list = [checkpoint]\n",
    "\n",
    "\t\tself.model.fit(self.train_x, self.train_y, batch_size=128, epochs=20, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\tdef load_model(self, filename):\n",
    "\t\t'''\n",
    "\t\tLoads the weights of an identically architectured neural net at the given filepath\n",
    "\t\t'''\n",
    "\t\tself.model.load_weights(filename)\n",
    "\t\tadam = optimizers.Adam(lr=self.learn_rate, decay=.001)\n",
    "\t\tself.model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\tdef predict_values(self, test_type='validation'):\n",
    "\t\t'''\n",
    "\t\tPredicts values based on training or validation data\n",
    "\t\tReturn:\n",
    "\t\t\tscores\n",
    "\t\t\tpredicted values\n",
    "\t\t'''\n",
    "\t\t# print(self.model.get_weights())\n",
    "\t\tif (test_type == 'validation'):\n",
    "\t\t\tscores = self.model.predict(self.test_x, verbose=True)\n",
    "\t\t\treturn scores, self.test_y\n",
    "\t\telif (test_type == 'training'):\n",
    "\t\t\tscores = self.model.predict(self.train_x, verbose=True)\n",
    "\t\t\treturn scores, self.train_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import util\n",
    "from nn import Collaborative_Filtering_Neural_Net\n",
    "\n",
    "def train():\n",
    "\t'''\n",
    "\ttrains a neural net and saves snapshots every epoch. Runs 20 epochs or until you quit the process.\n",
    "\t'''\n",
    "\ttrain_mat, val_mat, masks = util.k_cross()\n",
    "\tA = util.load_data_matrix()\n",
    "\n",
    "\tstart = time.time()\n",
    "\tnet = Collaborative_Filtering_Neural_Net(train_mat[0], val_mat[0], masks[0])\n",
    "\tnet.learn_rate=.1\n",
    "\tnet.construct_model(hidden_layer_pattern = 'exponential')\n",
    "\t# net.load_model('nn_model_exponential_one_hot_learn_rate_.1_lr_0.1_04.hdf5')\n",
    "\tnet.train_model(model_number='exponential_one_hot')\n",
    "\tprint('time taken to train in seconds:', time.time() - start)\n",
    "\n",
    "def test(model_name = '', test_type = 'validation'):\n",
    "\t'''\n",
    "\tGets the accuracy and validation error of a model.\n",
    "\tThis function assumes you have been saving your models\n",
    "\t'''\n",
    "\ttrain_mat, val_mat, masks = util.k_cross()\n",
    "\tA = util.load_data_matrix()\n",
    "\n",
    "\tnet = Collaborative_Filtering_Neural_Net(train_mat[0], val_mat[0], masks[0])\n",
    "\tnet.learn_rate=.1\n",
    "\tnet.construct_model(hidden_layer_pattern = 'exponential')\n",
    "\tnet.load_model(model_name)\n",
    "\n",
    "\tpred_scores , true_scores= net.predict_values(test_type = test_type)\n",
    "\tpred_scores = pred_scores.argmax(axis=1)\n",
    "\ttrue_scores    = true_scores.argmax(axis=1)\n",
    "\n",
    "\t#get Accuracy\n",
    "\tnum_correct = np.sum(pred_scores == true_scores)\n",
    "\taccuracy    = num_correct/pred_scores.shape[0]*100\n",
    "\n",
    "\t#get MSE\n",
    "\terror = pred_scores-true_scores\n",
    "\tmse   = np.mean(np.power(error, 2))\n",
    "\n",
    "\tprint('The {} accuracy of the model is {}%'.format(test_type, accuracy))\n",
    "\tprint('The {} mean squared error of the model is {}'.format(test_type, mse))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttrain()\n",
    "\ttest('nn_model_exponential_one_hot_round_2_lr_0.1_08.hdf5', test_type='training')\n",
    "\ttest('nn_model_exponential_one_hot_round_2_lr_0.1_08.hdf5', test_type='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
