{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation\n",
    "### A case study for collaborative filtering\n",
    "What do we talke about when we talk about collaborative filtering?\n",
    "We should clearly distinguish the following two tasks: (1) rating prediction and (2) top N recommendations.\n",
    "\n",
    "In this notebook, we will use the MovieLens Dataset to illustrate different approaches to build the recommender system.\n",
    "\n",
    "The MovieLens Dataset contains 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users.\n",
    "![title](Images/movielens-v0-v4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top N recommendation problem in terms of Deep Learning\n",
    "Consider a very simple example:\n",
    "\n",
    "“100 200 123/0 100 10 300/0 1 2 3 4 5/0”\n",
    "We can see that\n",
    "\n",
    "There are 3 users in our database\n",
    "The first user likes movies with the identifiers: 100 200 123\n",
    "The second: 100 10 300\n",
    "The third: 1 2 3 4 5\n",
    "“/0” — special symbol for separating different users\n",
    "User IDs are not important, only movie IDs (and its relative order) are important\n",
    "\n",
    "Afterwards, in theory, one can take the state of the art NLP model and train it to predict the next identifier in our “text”, which during the serve phase will represent the actual recommendation.\n",
    "\n",
    "\n",
    "#### Movies plus tags, putting it all together\n",
    "The hypothesis is that by allowing the user to operate both movies and tags, we speed up his way to a list of relevant movies reflecting his current mood.\n",
    "\n",
    "The task of constructing a neural network architecture, capable of working with the two mentioned entities, arises. See figure 1.\n",
    "\n",
    "![title](Images/lstm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the ratings data frame: (100836, 4)\n",
      "Shape of the tags data frame: (3683, 4)\n",
      "Shape of the movies data frame: (9742, 3)\n",
      "Shape of the sampled ratings data frame: (20167, 4)\n",
      "Shape of the sampled tags data frame: (737, 4)\n",
      "Shape of the sampled movies data frame: (9742, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split as sk_split\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "# Load datasets\n",
    "ratings = pd.read_csv('ml-latest/ratings.csv')\n",
    "print ('Shape of the ratings data frame:', ratings.shape)\n",
    "\n",
    "tags = pd.read_csv('ml-latest/tags.csv')\n",
    "print ('Shape of the tags data frame:', tags.shape)\n",
    "\n",
    "movies = pd.read_csv('ml-latest/movies.csv')\n",
    "print ('Shape of the movies data frame:', movies.shape)\n",
    "\n",
    "#Will take\n",
    "tags = tags.sample(frac=0.2)\n",
    "ratings = ratings.sample(frac=0.2)\n",
    "\n",
    "print ('Shape of the sampled ratings data frame:', ratings.shape)\n",
    "print ('Shape of the sampled tags data frame:', tags.shape)\n",
    "print ('Shape of the sampled movies data frame:', movies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20167.000000</td>\n",
       "      <td>20167.000000</td>\n",
       "      <td>20167.000000</td>\n",
       "      <td>2.016700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>326.388506</td>\n",
       "      <td>19689.470918</td>\n",
       "      <td>3.497149</td>\n",
       "      <td>1.207442e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>182.371921</td>\n",
       "      <td>35953.968273</td>\n",
       "      <td>1.040013</td>\n",
       "      <td>2.154415e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.281246e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>177.000000</td>\n",
       "      <td>1203.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.019572e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>325.000000</td>\n",
       "      <td>3033.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.186161e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>477.000000</td>\n",
       "      <td>7997.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.435994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>610.000000</td>\n",
       "      <td>193581.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.537757e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userId        movieId        rating     timestamp\n",
       "count  20167.000000   20167.000000  20167.000000  2.016700e+04\n",
       "mean     326.388506   19689.470918      3.497149  1.207442e+09\n",
       "std      182.371921   35953.968273      1.040013  2.154415e+08\n",
       "min        1.000000       1.000000      0.500000  8.281246e+08\n",
       "25%      177.000000    1203.500000      3.000000  1.019572e+09\n",
       "50%      325.000000    3033.000000      3.500000  1.186161e+09\n",
       "75%      477.000000    7997.000000      4.000000  1.435994e+09\n",
       "max      610.000000  193581.000000      5.000000  1.537757e+09"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display summary statistics about data\n",
    "ratings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print sample movies data\n",
    "movies.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop timestamp data from ratings as the time a movie was rates is not relevant for our purposes. We will keep the timestamp for tags however for two reasons:\n",
    "\n",
    "Different tags might be associated with a movie by the same user at the same time. Therefore if we split the database randomly, we could create a data leak where it would be easier for the algorithm to predict movie rating.\n",
    "Instead we will use time to split the data set into training and test datasets. This would be very applicable to a real world scenario where we have past data and we would like to predict ratings in the future for the users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.drop(['timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will merge tags and ratings data using inner join as the aim of our project is to determine how accurately we can predict movie rating using tags. Therefore we will not use ratings data without any tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the joint data frame: (108, 5)\n",
      "   userId  movieId  rating                tag   timestamp\n",
      "0     567   168252     4.0             gritty  1525283940\n",
      "1     567   168252     4.0               dark  1525283942\n",
      "2     474     7256     4.0  mountain climbing  1138039857\n",
      "3     474     3498     4.0   In Netflix queue  1137201900\n",
      "4     474      926     4.5          Hollywood  1137202821\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.merge(ratings, tags, how='inner')\n",
    "print ('Shape of the joint data frame:', data.shape)\n",
    "print (data.head(n=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean, Process and Examine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId 16\n",
      "movieId 87\n",
      "rating 9\n",
      "tag 100\n",
      "timestamp 105\n"
     ]
    }
   ],
   "source": [
    "for column in data.columns:\n",
    "    print (column, data[column].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 6500 unique users, 8500 movies and 13800 tags. \n",
    "There are also 10 available ratings a user can give which makes it possible for us to use classification methods. If there were alot more unique ratings so that target variable is continuous, we could have used regression models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "plt.hist(data['rating'],10)\n",
    "plt.xlabel('Movies'); \n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0,5)\n",
    "plt.title('Movie Ratings Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the histogram plot that the most frequent movie rating given is 4. \n",
    "This also results in an unbalanced target variable, therefore we will need to address this during machine learning. \n",
    "\n",
    "Let's visualize tags by average rating to see if get an initial feeling whether tags would be a good predictor variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>tag</th>\n",
       "      <th>Andrew Lloyd Weber</th>\n",
       "      <th>Arthur Miller</th>\n",
       "      <th>Australia</th>\n",
       "      <th>Bill Murray</th>\n",
       "      <th>Canada</th>\n",
       "      <th>Capote</th>\n",
       "      <th>Christmas</th>\n",
       "      <th>Coen Brothers</th>\n",
       "      <th>Creature Feature</th>\n",
       "      <th>Denzel Washington</th>\n",
       "      <th>...</th>\n",
       "      <th>tear jerker</th>\n",
       "      <th>tearjerking</th>\n",
       "      <th>teenage pregnancy</th>\n",
       "      <th>transplants</th>\n",
       "      <th>twist ending</th>\n",
       "      <th>unsettling</th>\n",
       "      <th>violence</th>\n",
       "      <th>visually appealing</th>\n",
       "      <th>wedding</th>\n",
       "      <th>weddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tag     Andrew Lloyd Weber  Arthur Miller  Australia  Bill Murray  Canada  \\\n",
       "rating                 2.0            4.0        4.5          3.5     4.0   \n",
       "\n",
       "tag     Capote  Christmas  Coen Brothers  Creature Feature  Denzel Washington  \\\n",
       "rating     3.5        4.5            3.5               3.0                5.0   \n",
       "\n",
       "tag       ...     tear jerker  tearjerking  teenage pregnancy  transplants  \\\n",
       "rating    ...             3.5          3.5                4.0          4.5   \n",
       "\n",
       "tag     twist ending  unsettling  violence  visually appealing  wedding  \\\n",
       "rating           4.5         3.0       4.0                 2.5      4.0   \n",
       "\n",
       "tag     weddings  \n",
       "rating       3.5  \n",
       "\n",
       "[1 rows x 100 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute average ratings by tag\n",
    "tagratings = pd.pivot_table(data, values=['rating'], columns=['tag'], aggfunc='mean')\n",
    "tagratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some tags have higher average ratings than others. We can also see that there are special characters that we can remove to have less number of unique tags and improve our performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove any special characters from tag\n",
    "#Relabel ratings \n",
    "data['rating'] = data['rating'].apply(lambda x: 1 if x > 4 else 0)\n",
    "\n",
    "#Delete special characters\n",
    "data['tag'] = data['tag'].apply(lambda x: str(x))\n",
    "data['tag'] = data['tag'].map(lambda x: re.sub(r'([^\\s\\w]|_)+', '', x))\n",
    "data['tag'] = data['tag'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 129 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['tag'])\n",
    "sequences = tokenizer.texts_to_sequences(data['tag'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "pseq = pad_sequences(sequences)\n",
    "pdseq = pd.DataFrame(pseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2018-05-02 10:59:00\n",
       "1     2018-05-02 10:59:02\n",
       "2     2006-01-23 10:10:57\n",
       "3     2006-01-13 17:25:00\n",
       "4     2006-01-13 17:40:21\n",
       "5     2006-01-13 18:25:33\n",
       "6     2006-01-13 18:17:14\n",
       "7     2016-09-22 17:56:47\n",
       "8     2006-01-23 10:05:40\n",
       "9     2006-02-13 05:49:33\n",
       "10    2006-02-13 05:49:33\n",
       "11    2006-01-13 17:32:22\n",
       "12    2016-03-12 20:28:18\n",
       "13    2006-01-15 17:34:56\n",
       "14    2016-03-12 21:06:36\n",
       "15    2016-03-12 21:06:27\n",
       "16    2006-01-23 10:15:55\n",
       "17    2006-01-23 10:15:55\n",
       "18    2006-01-23 09:59:41\n",
       "19    2006-01-23 08:13:11\n",
       "20    2006-01-15 15:35:40\n",
       "21    2006-02-23 06:02:24\n",
       "22    2018-05-05 14:18:58\n",
       "23    2006-01-17 10:13:41\n",
       "24    2006-01-23 10:17:34\n",
       "25    2006-01-15 17:14:55\n",
       "26    2006-01-23 10:13:31\n",
       "27    2010-03-28 20:16:37\n",
       "28    2010-07-09 06:58:32\n",
       "29    2010-03-28 20:16:52\n",
       "              ...        \n",
       "78    2006-01-15 17:23:01\n",
       "79    2006-01-15 17:23:07\n",
       "80    2007-04-16 16:16:33\n",
       "81    2006-01-15 17:13:11\n",
       "82    2006-01-26 12:27:16\n",
       "83    2006-01-13 14:27:22\n",
       "84    2006-01-15 17:09:54\n",
       "85    2018-05-02 10:49:51\n",
       "86    2016-03-12 21:21:46\n",
       "87    2018-06-23 11:06:38\n",
       "88    2015-02-16 18:18:49\n",
       "89    2015-02-16 18:18:35\n",
       "90    2015-07-02 10:12:30\n",
       "91    2015-07-02 10:12:27\n",
       "92    2006-01-23 09:55:49\n",
       "93    2006-01-13 18:26:12\n",
       "94    2018-05-02 10:58:37\n",
       "95    2006-01-13 14:32:52\n",
       "96    2006-01-13 11:37:46\n",
       "97    2006-01-24 13:24:09\n",
       "98    2016-03-13 13:39:35\n",
       "99    2006-01-15 17:39:15\n",
       "100   2006-01-17 10:05:24\n",
       "101   2006-10-25 11:14:01\n",
       "102   2006-01-15 17:16:20\n",
       "103   2006-01-13 18:21:10\n",
       "104   2006-01-13 18:27:16\n",
       "105   2006-01-13 17:11:07\n",
       "106   2006-01-26 12:27:04\n",
       "107   2006-01-13 11:46:13\n",
       "Name: timestamp, Length: 108, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert timestamp in seconds to datetime format\n",
    "data['timestamp'] = data['timestamp'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "data['timestamp'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in plot summary:  126\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english',decode_error='ignore', analyzer='word')\n",
    "corpus = data['tag'].values\n",
    "wordvec = vectorizer.fit_transform(corpus.ravel())\n",
    "wordvec = wordvec.toarray()\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "print(\"number of words in plot summary: \", len(words))\n",
    "pdwordvec = pd.DataFrame(wordvec,columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For deep learning with LSTM, we will only use word sequences\n",
    "dpdata = pd.concat([data, pdseq], axis=1)\n",
    "\n",
    "dpdata = dpdata.drop(['tag'], axis=1)\n",
    "dpdata = dpdata.drop(['userId'], axis=1)\n",
    "dpdata = dpdata.drop(['movieId'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model using Word Embeddings and LSTM\n",
    "\n",
    "Neural networks are coveted for their performance with huge amounts of data - suitable for tasks such as image recognition and NLP. I wanted to give it a shot using GloVe embeddings as a layer and LSTM to top it, ending with dense layer with softmax activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in test data set: 17\n",
      "Number of rows in training data set: 91\n"
     ]
    }
   ],
   "source": [
    "train = dpdata[(dpdata['timestamp'] < '2016-08-01') ]\n",
    "test = dpdata[(dpdata['timestamp'] >= '2016-08-01') ]\n",
    "\n",
    "print(\"Number of rows in test data set:\", (len(test)))\n",
    "print(\"Number of rows in training data set:\",(len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('ml-latest/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(words), 100))\n",
    "for i in range(len(words)):\n",
    "    embedding_vector = embeddings_index.get(words[i])\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "pdembedding = pd.DataFrame(embedding_matrix.T,columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['timestamp'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-639c28d69296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Define target and predictor variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#remove timestamp as it won't be needed for modeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/PY36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/PY36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   2560\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2563\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/PY36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3742\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3743\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3744\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3745\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3746\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['timestamp'] not contained in axis"
     ]
    }
   ],
   "source": [
    "#Define target and predictor variables\n",
    "#remove timestamp as it won't be needed for modeling\n",
    "train = train.drop(['timestamp'], axis=1)\n",
    "test = test.drop(['timestamp'], axis=1)\n",
    "y_train = train['rating']\n",
    "y_test = test['rating']\n",
    "x_train = train.drop(['rating'], axis=1)\n",
    "x_test = test.drop(['rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 100)         12600     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10)                4440      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 17,062\n",
      "Trainable params: 4,462\n",
      "Non-trainable params: 12,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "y_test_matrix = to_categorical(y_test)\n",
    "y_train_matrix = to_categorical(y_train)\n",
    "x_train_array = np.array(x_train)\n",
    "x_test_array = np.array(x_test)\n",
    "\n",
    "epochs = 20\n",
    "lrate = 0.01\n",
    "sgd = SGD(lr=lrate)\n",
    "early_stopping = EarlyStopping(monitor='acc',patience=2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)-3, 100, mask_zero=True, weights=[embedding_matrix],trainable=False))\n",
    "model.add(LSTM(10, return_sequences=False))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model \n",
    "model.fit(x_train_array, y_train_matrix, validation_data=(x_test_array, y_test_matrix), epochs=epochs, batch_size=250, class_weight='balanced', callbacks=[early_stopping, TQDMNotebookCallback()])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test_array, y_test_matrix, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3968/4023 [============================>.] - ETA: 0s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "y_predlstm = model.predict_proba(x_test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that deep learning model predicts ratings by user/movie using just the tags with 59% accuracy. Let's see if we can use the deep learning prediction probabilities to improve the performance of our best performing machine learning model: logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model of Deep Learning and Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate weighted probabilities \n",
    "y_pred = (0.4*y_predlstm + 0.6*y_predlog)\n",
    "\n",
    "#Predict ratings using the weighted probabilities\n",
    "y_predensem = np.zeros((len(y_pred)))\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i,1] >= 0.5:\n",
    "        y_predensem[i] = 1\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy of the test set for Ensemble model is: \", np.round(accuracy_score(y_test, y_predensem),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the test set for Ensemble model is: 0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "My objective was to predict the rating of a movie by user, using tags attributed to movies by users. As a model to predict whether a user would really like a movie, I grouped 4.5 & 5 scores together versus the others. For this purpose I used full set of the data provided by MovieLens and used an inner join to eliminate any ratings data that did not have any tags. \n",
    "\n",
    "After processing and cleaning the data, I used Logistic Regression, Random Forest, Multinomial Naive Bayes,Ada Boost and XGBoost as machine learning algorithms to classify the rating of a movie using userId, movieId, movie genres and tags clustered using GloVe word embeddings. \n",
    "\n",
    "The algorithm that had the best classification accuracy was Logistic Regression with 66%. The deep learning model using GloVe word embeddings and LSTM achieved 59% accuracy. The ensemble model that incorporates weighted probabilities of these two models achieved 67% which achieved 11% better accuracy than the baseline model.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
