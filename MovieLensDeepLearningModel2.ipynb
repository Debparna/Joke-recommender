{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adopted from https://github.com/alexvlis/movie-recommendation-system/blob/master/util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(31, 2.5), (1029, 3.0), (1061, 3.0), (1129, 2.0), (1172, 4.0), (1263, 2.0), (1287, 2.0), (1293, 2.0), (1339, 3.5), (1343, 2.0), (1371, 2.5), (1405, 1.0), (1953, 4.0), (2105, 4.0), (2150, 3.0), (2193, 2.0), (2294, 2.0), (2455, 2.5), (2968, 1.0), (3671, 3.0)]\n",
      "(671, 9066)\n",
      "Number of ratings = 100004\n",
      "Total entries = 6083286\n",
      "Sparsity = 1.6439141608663477%\n",
      "(671, 9066)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#we're looking to populate these 4 dictionaries\n",
    "movieId_movieName = {}\n",
    "movieId_movieCol  = {}\n",
    "userId_userRow    = {}\n",
    "userId_rating     = {}\n",
    "\n",
    "#for reference\n",
    "movieId_isRated   = {}\n",
    "\n",
    "#then populate a data matrix based on the userId_rating dictionary\n",
    "\n",
    "\n",
    "'''\n",
    "Read Basic Movie Info\n",
    "'''\n",
    "\n",
    "path     = 'data'\n",
    "filename = 'movies.csv'\n",
    "\n",
    "\n",
    "dataFrame = pd.read_csv('{}/{}'.format(path,filename))\n",
    "\n",
    "for i in range(len(dataFrame['movieId'])):\n",
    "    movieId = dataFrame['movieId'][i]\n",
    "    \n",
    "    movieId_movieName[movieId] = dataFrame['title'][i]    \n",
    "    movieId_isRated[movieId]    = 0\n",
    "\n",
    "   \n",
    "'''\n",
    "Read in the ratings\n",
    "'''\n",
    "path     = 'data'\n",
    "filename = 'ratings.csv'\n",
    "\n",
    "\n",
    "dataFrame = pd.read_csv('{}/{}'.format(path,filename))\n",
    "\n",
    "for i in range(len(dataFrame)):\n",
    "    userId  = dataFrame['userId'][i]\n",
    "    movieId = dataFrame['movieId'][i]\n",
    "    rating  = dataFrame['rating'][i]\n",
    "    \n",
    "    if userId not in userId_rating.keys():\n",
    "        userId_rating[userId] = [(movieId, rating)]\n",
    "    else:\n",
    "        userId_rating[userId].append((movieId, rating))\n",
    "    \n",
    "    movieId_isRated[movieId] = 1\n",
    "        \n",
    "print(userId_rating[1])\n",
    "    \n",
    "\n",
    "for movieId, isRated in movieId_isRated.items():\n",
    "    if isRated == 0:\n",
    "        del movieId_movieName[movieId]\n",
    "        \n",
    "userId_userRow\n",
    "movieId_movieCol\n",
    "\n",
    "i = 0\n",
    "for movieId in sorted(movieId_movieName):\n",
    "    movieId_movieCol[movieId] = i\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for userId in sorted(userId_rating):\n",
    "    userId_userRow[userId] = i\n",
    "    i+=1\n",
    "\n",
    "m = len(userId_userRow.keys())\n",
    "n = len(movieId_movieCol.keys())\n",
    "A = np.zeros((m,n))\n",
    "    \n",
    "print(A.shape)\n",
    "for userId, ratings in userId_rating.items():\n",
    "    for rating in ratings:\n",
    "        movieId   = rating[0]\n",
    "        score     = rating[1]\n",
    "        \n",
    "        if (userId in userId_userRow and movieId in movieId_movieCol):\n",
    "            i = userId_userRow[userId]\n",
    "\n",
    "            j = movieId_movieCol[movieId]\n",
    "            A[i,j] = score\n",
    "\n",
    "ratingCount = 0\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        if (A[i][j] != 0):\n",
    "#             if(ratingCount < 20):\n",
    "#                 print(A[i][j])\n",
    "            ratingCount += 1\n",
    "\n",
    "\n",
    "print('Number of ratings = {}'.format(ratingCount))\n",
    "print('Total entries = {}'.format(m*n))\n",
    "print('Sparsity = {}%'.format(ratingCount*100/(m*n)))\n",
    "\n",
    "d = {'movieId_movieName': movieId_movieName,\n",
    "     'movieId_movieCol' : movieId_movieCol,\n",
    "     'userId_userRow'   : userId_userRow,\n",
    "     'userId_rating'    : userId_rating }\n",
    "pickle.dump(A, open('data/data_matrix.p', 'wb'))\n",
    "pickle.dump(d, open('data/data_dicts.p', 'wb'))\n",
    "print (A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape = (671, 9066)\n",
      "MSE = 13.742275772422758\n",
      "you wasted 40.15754175186157 seconds of my life\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "def k_cross(k = 10):\n",
    "\n",
    "\tA = load_data_matrix()\n",
    "\tm = A.shape[0]\n",
    "\tn = A.shape[1]\n",
    "\n",
    "\tprint('A.shape = {}'.format(A.shape))\n",
    "\n",
    "\tprediction_matrices = []\n",
    "\ttraining_matrices   = []\n",
    "\tindex_lists         = []\n",
    "\tfor i in range(k):\n",
    "\t    A_copy = A.copy()\n",
    "\t    prediction_matrices.append(np.zeros((m, n)))\n",
    "\t    training_matrices.append(A_copy)\n",
    "\t    index_lists.append(np.zeros((m, n), dtype=bool))\n",
    "\n",
    "\tit    = 0\n",
    "\tfor i in range(A.shape[0]):\n",
    "\t    for j in range(A.shape[1]):\n",
    "\t        if (A[i, j] != 0):\n",
    "\t            training_matrices[it%k][i, j]   = 0\n",
    "\t            prediction_matrices[it%k][i, j] = A[i, j]\n",
    "\t            index_lists[it%k][i, j] = True\n",
    "\t            it+=1\n",
    "\n",
    "\treturn training_matrices, prediction_matrices, index_lists\n",
    "\n",
    "def load_data_matrix(filename='data_matrix.p', path='data'):\n",
    "\tfilepath = filename if path == '' else '{}/{}'.format(path,filename)\n",
    "\tA = pickle.load( open('{}'.format(filepath), 'rb'))\n",
    "\treturn A\n",
    "\n",
    "def get_MSE(mat1, mask, mat2=''):\n",
    "\tif (mat2 == ''):\n",
    "\t\tmat2 = load_data_matrix()\n",
    "\n",
    "\tA_mask    = mat2[mask]\n",
    "\tmat1_mask = mat1[mask]\n",
    "\n",
    "\tdiff = A_mask-mat1_mask\n",
    "\tmse = np.dot(diff, diff)/A_mask.shape\n",
    "\treturn mse[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tk = 10\n",
    "\n",
    "\ttrain_mats, val_mats, masks = k_cross(k=k)\n",
    "\tprint('MSE = {}'.format(get_MSE(train_mats[0], masks[0])))\n",
    "\n",
    "\tm = train_mats[0].shape[0]\n",
    "\tn = train_mats[0].shape[1]\n",
    "\tstart = time.time()\n",
    "\tfor i in range(m):\n",
    "\t    for j in range(n):\n",
    "\t        for index in range(k):\n",
    "\t            if(train_mats[index][i,j] != 0 and val_mats[index][i,j] != 0):\n",
    "\t                print('we have a problem')\n",
    "\tend = time.time()\n",
    "\tprint('you wasted {} seconds of my life'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "fitting item: 67054 114 171 174208 322 350 396 478 488 579 582 604\n",
      "Done.\n",
      "Sense and Sensibility (1995) , rating: 5.0\n",
      "Clueless (1995) , rating: 5.0\n",
      "Apollo 13 (1995) , rating: 5.0\n",
      "Circle of Friends (1995) , rating: 5.0\n",
      "Like Water for Chocolate (Como agua para chocolate) (1992) , rating: 5.0\n",
      "Legends of the Fall (1994) , rating: 5.0\n",
      "Nightmare Before Christmas, The (1993) , rating: 5.0\n",
      "Brady Bunch Movie, The (1995) , rating: 5.0\n",
      "Terminator 2: Judgment Day (1991) , rating: 5.0\n",
      "Dances with Wolves (1990) , rating: 5.0\n",
      "Batman (1989) , rating: 5.0\n",
      "\n",
      "Recommendations\n",
      "Clerks (1994)\n",
      "Shallow Grave (1994)\n",
      "Naked Gun 33 1/3: The Final Insult (1994)\n",
      "Santa Clause, The (1994)\n",
      "Highlander III: The Sorcerer (a.k.a. Highlander: The Final Dimension) (1994)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import util\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class CollaborativeFiltering():\n",
    "\t''' Collaborative Filtering Estimator '''\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef pearsonr(r_a, r_i):\n",
    "\t\t'''\n",
    "\t\tinput: rating vectors of user a (active user) and user i\n",
    "\t\toutput: pearsor correlation coefficient value\n",
    "\t\t'''\n",
    "\n",
    "\t\t# Get movies both users rated\n",
    "\t\tmask = np.logical_and(r_a>0, r_i>0)\n",
    "\t\tif mask.sum() == 0:\n",
    "\t\t\treturn -1 # Does not actually matter what value we return\n",
    "\n",
    "\t\tr_a = r_a[mask]\n",
    "\t\tr_i = r_i[mask]\n",
    "\n",
    "\t\t# Get mean rating for each user\n",
    "\t\tr_a_bar = np.mean(r_a)\n",
    "\t\tr_i_bar = np.mean(r_i)\n",
    "\n",
    "\t\t# Calculate pearson correlation coefficient\n",
    "\t\treturn  np.dot(r_a-r_a_bar, r_i-r_i_bar) /         \\\n",
    "\t\t\t\tnp.sqrt(np.dot(r_a-r_a_bar, r_a-r_a_bar) * \\\n",
    "\t\t\t\t\t\tnp.dot(r_i-r_i_bar, r_i-r_i_bar))  \\\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef significance(r_a, r_i, thresh):\n",
    "\t\t'''\n",
    "\t\tinput: rating vectors of user a (active user) and user i\n",
    "\t\toutput: significance weight\n",
    "\t\t'''\n",
    "\t\tS = np.logical_and(r_a>0, r_i>0).sum()\n",
    "\t\tif S > thresh:\n",
    "\t\t\treturn 1\n",
    "\t\treturn S/thresh\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef prediction(r, w):\n",
    "\t\t'''\n",
    "\t\tinput: neighborhood matrix of k rows, weight vector of neighborhood\n",
    "\t\toutput: offset prediction vector for active user\n",
    "\t\t'''\n",
    "\t\treturn np.dot((r.T - r.mean(axis=1)), w) / np.sum(w)\n",
    "\n",
    "\n",
    "\t'''*************************** Class methods ****************************'''\n",
    "\n",
    "\tdef __init__(self, method=\"neighborhood\", k=10, s=50):\n",
    "\t\tself.method = method\n",
    "\t\tself.k = k\n",
    "\t\tself.s = s\n",
    "\n",
    "\tdef fit(self, A, verbose=False):\n",
    "\t\tself.verbose = verbose\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(\"Training...\")\n",
    "\n",
    "\t\tif self.method == \"neighborhood\":\n",
    "\t\t\treturn self.neighborhood_based(A)\n",
    "\t\tif self.method == \"item\":\n",
    "\t\t\treturn self.neighborhood_based(A.T).T\n",
    "\n",
    "\n",
    "\t'''*************************** Private methods **************************'''\n",
    "\n",
    "\tdef neighborhood_based(self, A):\n",
    "\t\tA_new = np.array(A) # copy A matrix\n",
    "\n",
    "\t\tfor a, r_a in enumerate(A):\n",
    "\t\t\t# weight vector for active user a\n",
    "\t\t\tw = np.zeros(A.shape[0])\n",
    "\t\t\tw[a] = -1 # ignore active user\n",
    "\n",
    "\t\t\tfor i, r_i in enumerate(A):\n",
    "\t\t\t\tif i == a:\n",
    "\t\t\t\t\t# Skip active user\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tw[i] = CollaborativeFiltering.pearsonr(r_a, r_i) * \\\n",
    "\t\t\t\t\t\tCollaborativeFiltering.significance(r_a, r_i, self.s)\n",
    "\n",
    "\t\t\t# Get indices of neighborhood\n",
    "\t\t\tK = np.argsort(w)[:self.k]\n",
    "\t\t\tmask = r_a==0\n",
    "\t\t\tA_new[a, mask] = (np.mean(r_a[r_a>0]) + CollaborativeFiltering.prediction(A[K], w[K]))[mask]\n",
    "\n",
    "\t\t\tif self.verbose:\n",
    "\t\t\t\tprint(\"fitting item:\", a, end='\\r')\n",
    "\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(\"\\nDone.\")\n",
    "\n",
    "\t\tA_new[A_new>5] = 5.0 # clip all ratings to 5\n",
    "\t\treturn np.around(A_new*2)/2 # round to nearest .5\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\tA = util.load_data_matrix()\n",
    "\tcf = CollaborativeFiltering()\n",
    "\tA_new = cf.fit(A, verbose=True)\n",
    "\trecommendations = np.argsort(A_new[1, :])[:5]\n",
    "\n",
    "\tB = pickle.load( open('{}'.format('data/data_dicts.p'), 'rb'))\n",
    "\n",
    "\tfor movie_id,rating in B['userId_rating'][2]:\n",
    "\t   if rating == 5 :\n",
    "\t       print(B['movieId_movieName'][movie_id] , \", rating:\" , rating )\n",
    "\n",
    "\tl = recommendations\n",
    "\tk_list =[]\n",
    "\tfor movie_column in l :\n",
    "\t   for k, v in B['movieId_movieCol'].items():\n",
    "\t       if v == movie_column:\n",
    "\t           k_list.append(k)\n",
    "\tprint(\"\")\n",
    "\tprint(\"Recommendations\")\n",
    "\tfor movie_id in k_list :\n",
    "\t   print(B['movieId_movieName'][movie_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class Collaborative_Filtering_Neural_Net(object):\n",
    "\n",
    "\tdef __init__(self, train_data, val_data, mask, num_layers=3, learn_rate=.2):\n",
    "\n",
    "\t\tself.train_data = train_data\n",
    "\t\tself.val_data   = val_data\n",
    "\t\tself.mask       = mask\n",
    "\t\tself.num_layers = num_layers\n",
    "\n",
    "\t\tself.m          = self.train_data.shape[0]\n",
    "\t\tself.n \t\t\t= self.train_data.shape[1]\n",
    "\t\t\n",
    "\t\tself.learn_rate = learn_rate\n",
    "\n",
    "\t\tself.construct_input()\n",
    "\n",
    "\n",
    "\tdef construct_input(self):\n",
    "\t\t'''\n",
    "\t\tConstruct training input/output from the training data matrix\n",
    "\t\tand \n",
    "\t\tConstruct validation input/output from the training/validation \n",
    "\t\t'''\n",
    "\t\tdef change_to_one_hot(value, value_range):\n",
    "\t\t\tone_hot_vec = np.zeros(len(value_range))\n",
    "\t\t\tone_hot_vec[int(value/.5)] = 1\n",
    "\t\t\treturn one_hot_vec\n",
    "\n",
    "\t\tm = self.m\n",
    "\t\tn = self.n\n",
    "\n",
    "\t\tuser_indices, movie_indices = (np.where(self.train_data > 0))\n",
    "\t\tscores = self.train_data[self.mask]\n",
    "\n",
    "\t\tnum_train_samples = user_indices.shape[0]\n",
    "\n",
    "\t\tself.train_x = np.zeros((num_train_samples, m+n))\n",
    "\t\tself.train_y = np.zeros((num_train_samples, 11))\n",
    "\n",
    "\t\tstart = time.time()\n",
    "\n",
    "\t\t#construct training input and output X, y\n",
    "\t\tfor i in range(num_train_samples):\n",
    "\t\t\tu_ind = user_indices[i]\n",
    "\t\t\tm_ind = movie_indices[i]\n",
    "\n",
    "\t\t\tself.train_x[i, u_ind]   = 1\n",
    "\t\t\tself.train_x[i, m+m_ind] = 1\n",
    "\n",
    "\t\t\tscore \t\t\t= self.train_data[u_ind, m_ind]\n",
    "\t\t\tself.train_y[i] = change_to_one_hot(score, np.arange(0,5.5,.5))\n",
    "\n",
    "\t\t#construct test inputs for where we need to predict values\n",
    "\t\tuser_indices, movie_indices = np.where(self.mask)\n",
    "\t\tnum_test_samples = user_indices.shape[0]\n",
    "\t\tself.test_x = np.zeros((num_test_samples, m+n))\n",
    "\t\tself.test_y = np.zeros((num_test_samples, 11))\n",
    "\n",
    "\t\tfor i in range(num_test_samples):\n",
    "\t\t\tu_ind = user_indices[i]\n",
    "\t\t\tm_ind = movie_indices[i]\n",
    "\n",
    "\t\t\tself.test_x[i, u_ind]   = 1\n",
    "\t\t\tself.test_x[i, m+m_ind] = 1\n",
    "\n",
    "\t\t\tscore \t\t   = self.val_data[u_ind, m_ind]\n",
    "\t\t\tself.test_y[i] = change_to_one_hot(score, np.arange(0,5.5,.5))\n",
    "\n",
    "\t\tprint(time.time() - start)\n",
    "\n",
    "\n",
    "\tdef construct_model(self, hidden_layer_pattern = 'exponential'):\n",
    "\t\t'''\n",
    "\t\tConstructs a Neural network with a given pattern.\n",
    "\t\tThe pattern indicates how many neurons should exist at every layer.\n",
    "\t\tParam:\n",
    "\t\t\thidden_layer_pattern - The input layer and output layer are fixed, but the rate at which the layer sizes\n",
    "\t\t\tdecreases depends on the parameter, hidden_layer_pattern\n",
    "\t\t'''\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinput_size = self.m + self.n\n",
    "\t\t\n",
    "\t\t# add the first layer\n",
    "\t\tmodel.add(Dense(input_size, activation='relu', input_shape=(input_size,)))\n",
    "\n",
    "\t\t#one of the two model architectures tested\n",
    "\t\tif (hidden_layer_pattern == 'linear'):\n",
    "\t\t\tlinear_decrease = int(input_size/self.num_layers)\n",
    "\t\t\tfor i in range(self.num_layers):\n",
    "\t\t\t\tinput_size = input_size - linear_decrease\n",
    "\t\t\t\tmodel.add(Dense(input_size, activation='relu') )\n",
    "\n",
    "\t\tif (hidden_layer_pattern == 'exponential'):\n",
    "\t\t\texponential_decrease = int((np.exp(np.log(input_size)/(self.num_layers+2))))\n",
    "\t\t\tprint(exponential_decrease)\n",
    "\t\t\tfor i in range(self.num_layers):\n",
    "\t\t\t\tinput_size = int(input_size/exponential_decrease);\n",
    "\t\t\t\tmodel.add(Dense(input_size, activation='relu') )\n",
    "\n",
    "\t\tprint (model.output_shape)\n",
    "\t\t#one hot encoded output\n",
    "\t\tmodel.add(Dense(11, activation='relu'))\n",
    "\n",
    "\n",
    "\t\t# model says they optimized the log loss error\n",
    "\n",
    "\t\tadam = optimizers.Adam(lr=self.learn_rate, decay=.001)\n",
    "\t\tmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\t\tself.model = model\n",
    "\n",
    "\tdef train_model(self, model_number = 0):\n",
    "\t\t'''\n",
    "\t\tTrains the model. Saves checkpoints of the model at every epoch.\n",
    "\t\tI personally just stop training when I find that the loss function has barely changed. Since it takes\n",
    "\t\tso long to perform each epoch on my computer, I just keep running a 20 epoch train, stop it when I\n",
    "\t\thave to, then train again later.\n",
    "\t\tParam:\n",
    "\t\t\tmodel_number - Just changes the filename that the model is saved to. \n",
    "\t\t\t\t\t\t   Don't want to overwrite good save files during training, do you?\n",
    "\n",
    "\t\tNote: these checkpoints are 1GB each.\n",
    "\t\t'''\n",
    "\t\t# lets make checkpoints\n",
    "\t\tfilepath = \"nn_model_{}_lr_{}\".format(model_number,self.learn_rate)\n",
    "\t\tfilepath+= \"_{epoch:02d}.hdf5\"\n",
    "\n",
    "\t\tprint('learn_rate = {}'.format(self.learn_rate))\n",
    "\t\tcheckpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\t\tcallbacks_list = [checkpoint]\n",
    "\n",
    "\t\tself.model.fit(self.train_x, self.train_y, batch_size=128, epochs=5, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\tdef load_model(self, filename):\n",
    "\t\t'''\n",
    "\t\tLoads the weights of an identically architectured neural net at the given filepath\n",
    "\t\t'''\n",
    "\t\tself.model.load_weights(filename)\n",
    "\t\tadam = optimizers.Adam(lr=self.learn_rate, decay=.001)\n",
    "\t\tself.model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\tdef predict_values(self, test_type='validation'):\n",
    "\t\t'''\n",
    "\t\tPredicts values based on training or validation data\n",
    "\t\tReturn:\n",
    "\t\t\tscores\n",
    "\t\t\tpredicted values\n",
    "\t\t'''\n",
    "\t\t# print(self.model.get_weights())\n",
    "\t\tif (test_type == 'validation'):\n",
    "\t\t\tscores = self.model.predict(self.test_x, verbose=True)\n",
    "\t\t\treturn scores, self.test_y\n",
    "\t\telif (test_type == 'training'):\n",
    "\t\t\tscores = self.model.predict(self.train_x, verbose=True)\n",
    "\t\t\treturn scores, self.train_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape = (671, 9066)\n",
      "4.032506704330444\n",
      "6\n",
      "(None, 45)\n",
      "learn_rate = 0.1\n",
      "Epoch 1/20\n",
      " 1664/90003 [..............................] - ETA: 47:47 - loss: 4.7320 - acc: 0.2686"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import util\n",
    "from nn import Collaborative_Filtering_Neural_Net\n",
    "\n",
    "def train():\n",
    "\t'''\n",
    "\ttrains a neural net and saves snapshots every epoch. Runs 20 epochs or until you quit the process.\n",
    "\t'''\n",
    "\ttrain_mat, val_mat, masks = util.k_cross()\n",
    "\tA = util.load_data_matrix()\n",
    "\n",
    "\tstart = time.time()\n",
    "\tnet = Collaborative_Filtering_Neural_Net(train_mat[0], val_mat[0], masks[0])\n",
    "\tnet.learn_rate=.1\n",
    "\tnet.construct_model(hidden_layer_pattern = 'exponential')\n",
    "\t# net.load_model('nn_model_exponential_one_hot_learn_rate_.1_lr_0.1_04.hdf5')\n",
    "\tnet.train_model(model_number='exponential_one_hot')\n",
    "\tprint('time taken to train in seconds:', time.time() - start)\n",
    "\n",
    "def test(model_name = '', test_type = 'validation'):\n",
    "\t'''\n",
    "\tGets the accuracy and validation error of a model.\n",
    "\tThis function assumes you have been saving your models\n",
    "\t'''\n",
    "\ttrain_mat, val_mat, masks = util.k_cross()\n",
    "\tA = util.load_data_matrix()\n",
    "\n",
    "\tnet = Collaborative_Filtering_Neural_Net(train_mat[0], val_mat[0], masks[0])\n",
    "\tnet.learn_rate=.1\n",
    "\tnet.construct_model(hidden_layer_pattern = 'exponential')\n",
    "\tnet.load_model(model_name)\n",
    "\n",
    "\tpred_scores , true_scores= net.predict_values(test_type = test_type)\n",
    "\tpred_scores = pred_scores.argmax(axis=1)\n",
    "\ttrue_scores    = true_scores.argmax(axis=1)\n",
    "\n",
    "\t#get Accuracy\n",
    "\tnum_correct = np.sum(pred_scores == true_scores)\n",
    "\taccuracy    = num_correct/pred_scores.shape[0]*100\n",
    "\n",
    "\t#get MSE\n",
    "\terror = pred_scores-true_scores\n",
    "\tmse   = np.mean(np.power(error, 2))\n",
    "\n",
    "\tprint('The {} accuracy of the model is {}%'.format(test_type, accuracy))\n",
    "\tprint('The {} mean squared error of the model is {}'.format(test_type, mse))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttrain()\n",
    "\ttest('nn_model_exponential_one_hot_round_2_lr_0.1_08.hdf5', test_type='training')\n",
    "\ttest('nn_model_exponential_one_hot_round_2_lr_0.1_08.hdf5', test_type='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
