{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Merge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-88b6fb0e15a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCFModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Merge'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Embedding, Reshape, Merge\n",
    "from keras.models import Sequential\n",
    "\n",
    "class CFModel(Sequential):\n",
    "\n",
    "    # The constructor for the class\n",
    "    def __init__(self, n_users, m_items, k_factors, **kwargs):\n",
    "        # P is the embedding layer that creates an User by latent factors matrix.\n",
    "        # If the intput is a user_id, P returns the latent factor vector for that user.\n",
    "        P = Sequential()\n",
    "        P.add(Embedding(n_users, k_factors, input_length=1))\n",
    "        P.add(Reshape((k_factors,)))\n",
    "\n",
    "        # Q is the embedding layer that creates a Movie by latent factors matrix.\n",
    "        # If the input is a movie_id, Q returns the latent factor vector for that movie.\n",
    "        Q = Sequential()\n",
    "        Q.add(Embedding(m_items, k_factors, input_length=1))\n",
    "        Q.add(Reshape((k_factors,)))\n",
    "\n",
    "        super(CFModel, self).__init__(**kwargs)\n",
    "        \n",
    "        # The Merge layer takes the dot product of user and movie latent factor vectors to return the corresponding rating.\n",
    "        self.add(Merge([P, Q], mode='dot', dot_axes=1))\n",
    "\n",
    "    # The rate function to predict user's rating of unrated items\n",
    "    def rate(self, user_id, item_id):\n",
    "        return self.predict([np.array([user_id]), np.array([item_id])])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "Loaded the 3 datasets into 3 dataframes: *ratings*, *users*, and *movies*. \n",
    "Set *max_userid* as the max value of user_id in the ratings and *max_movieid* as the max value of movie_id in the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "# Reading ratings file\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'user_emb_id', 'movie_emb_id', 'rating'])\n",
    "max_userid = ratings['user_id'].drop_duplicates().max()\n",
    "max_movieid = ratings['movie_id'].drop_duplicates().max()\n",
    "\n",
    "# Reading ratings file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading ratings file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "'''\n",
    "# Randomly sample 1% of the ratings dataset\n",
    "small_ratings = ratings.sample(frac=0.02)\n",
    "small_users = max_userid.sample(frac=0.02)\n",
    "small_movies = max_movieid.sample(frac=0.02)\n",
    "\n",
    "# Randomly sample 20% of the ratings dataset\n",
    "small_ratings = ratings.sample(frac=0.20)\n",
    "small_users = max_userid.sample(frac=0.20)\n",
    "small_movies = max_movieid.sample(frac=0.20)\n",
    "\n",
    "# Randomly sample 60% of the ratings dataset\n",
    "small_ratings = ratings.sample(frac=0.60)\n",
    "small_users = max_userid.sample(frac=0.60)\n",
    "small_movies = max_movieid.sample(frac=0.60)\n",
    "'''\n",
    "\n",
    "# Create training set\n",
    "shuffled_ratings = ratings.sample(frac=1.)\n",
    "\n",
    "# Shuffling users\n",
    "Users = shuffled_ratings['user_emb_id'].values\n",
    "print ('Users:', Users, ', shape =', Users.shape)\n",
    "\n",
    "# Shuffling movies\n",
    "Movies = shuffled_ratings['movie_emb_id'].values\n",
    "print ('Movies:', Movies, ', shape =', Movies.shape)\n",
    "\n",
    "# Shuffling ratings\n",
    "Ratings = shuffled_ratings['rating'].values\n",
    "print ('Ratings:', Ratings, ', shape =', Ratings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras libraries\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Embedding, Reshape, Merge\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Merge\n",
    "\n",
    "# Define constants\n",
    "K_FACTORS = 100 # The number of dimensional embeddings for movies and users\n",
    "TEST_USER = 2000 # A random test user (user_id = 2000)\n",
    "\n",
    "# Define model\n",
    "model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "\n",
    "# Compile the model using MSE as the loss function and the AdaMax learning algorithm\n",
    "model.compile(loss='mse', optimizer='adamax')\n",
    "\n",
    "# Callbacks monitor the validation loss\n",
    "# Save the model weights each time the validation loss has improved\n",
    "callbacks = [EarlyStopping('val_loss', patience=2), ModelCheckpoint('weights.h5', save_best_only=True)]\n",
    "\n",
    "# Use 30 epochs, 90% training data, 10% validation data \n",
    "history = model.fit([Users, Movies], Ratings, nb_epoch=30, validation_split=.1, verbose=2, callbacks=callbacks)\n",
    "\n",
    "# Use 100 epochs, 90% training data, 10% validation data \n",
    "#history = model.fit([Users, Movies], Ratings, nb_epoch=100, validation_split=.1, verbose=2, callbacks=callbacks)\n",
    "\n",
    "# Show the best validation RMSE\n",
    "min_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history.history['val_loss']))\n",
    "print 'Minimum RMSE at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(math.sqrt(min_val_loss))\n",
    "\n",
    "# Use the pre-trained model\n",
    "trained_model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "\n",
    "# Load weights\n",
    "trained_model.load_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class Collaborative_Filtering_Neural_Net(object):\n",
    "\n",
    "\tdef __init__(self, train_data, val_data, mask, num_layers=3, learn_rate=.2):\n",
    "\n",
    "\t\tself.train_data = train_data\n",
    "\t\tself.val_data   = val_data\n",
    "\t\tself.mask       = mask\n",
    "\t\tself.num_layers = num_layers\n",
    "\n",
    "\t\tself.m          = self.train_data.shape[0]\n",
    "\t\tself.n \t\t\t= self.train_data.shape[1]\n",
    "\t\t\n",
    "\t\tself.learn_rate = learn_rate\n",
    "\n",
    "\t\tself.construct_input()\n",
    "\n",
    "\n",
    "\tdef construct_input(self):\n",
    "\t\t'''\n",
    "\t\tConstruct training input/output from the training data matrix\n",
    "\t\tand \n",
    "\t\tConstruct validation input/output from the training/validation \n",
    "\t\t'''\n",
    "\t\tdef change_to_one_hot(value, value_range):\n",
    "\t\t\tone_hot_vec = np.zeros(len(value_range))\n",
    "\t\t\tone_hot_vec[int(value/.5)] = 1\n",
    "\t\t\treturn one_hot_vec\n",
    "\n",
    "\n",
    "\t\tm = self.m\n",
    "\t\tn = self.n\n",
    "\n",
    "\t\tuser_indices, movie_indices = (np.where(self.train_data > 0))\n",
    "\t\tscores = self.train_data[self.mask]\n",
    "\n",
    "\t\tnum_train_samples = user_indices.shape[0]\n",
    "\n",
    "\t\tself.train_x = np.zeros((num_train_samples, m+n))\n",
    "\t\tself.train_y = np.zeros((num_train_samples, 11))\n",
    "\n",
    "\t\tstart = time.time()\n",
    "\n",
    "\t\t#construct training input and output X, y\n",
    "\t\tfor i in range(num_train_samples):\n",
    "\t\t\tu_ind = user_indices[i]\n",
    "\t\t\tm_ind = movie_indices[i]\n",
    "\n",
    "\t\t\tself.train_x[i, u_ind]   = 1\n",
    "\t\t\tself.train_x[i, m+m_ind] = 1\n",
    "\n",
    "\t\t\tscore \t\t\t= self.train_data[u_ind, m_ind]\n",
    "\t\t\tself.train_y[i] = change_to_one_hot(score, np.arange(0,5.5,.5))\n",
    "\n",
    "\n",
    "\n",
    "\t\t#construct test inputs for where we need to predict values\n",
    "\t\tuser_indices, movie_indices = np.where(self.mask)\n",
    "\t\tnum_test_samples = user_indices.shape[0]\n",
    "\t\tself.test_x = np.zeros((num_test_samples, m+n))\n",
    "\t\tself.test_y = np.zeros((num_test_samples, 11))\n",
    "\n",
    "\t\tfor i in range(num_test_samples):\n",
    "\t\t\tu_ind = user_indices[i]\n",
    "\t\t\tm_ind = movie_indices[i]\n",
    "\n",
    "\t\t\tself.test_x[i, u_ind]   = 1\n",
    "\t\t\tself.test_x[i, m+m_ind] = 1\n",
    "\n",
    "\t\t\tscore \t\t   = self.val_data[u_ind, m_ind]\n",
    "\t\t\tself.test_y[i] = change_to_one_hot(score, np.arange(0,5.5,.5))\n",
    "\n",
    "\t\tprint(time.time() - start)\n",
    "\n",
    "\n",
    "\tdef construct_model(self, hidden_layer_pattern = 'exponential'):\n",
    "\t\t'''\n",
    "\t\tConstructs a Neural network with a given pattern.\n",
    "\t\tThe pattern indicates how many neurons should exist at every layer.\n",
    "\t\tParam:\n",
    "\t\t\thidden_layer_pattern - The input layer and output layer are fixed, but the rate at which the layer sizes\n",
    "\t\t\tdecreases depends on the parameter, hidden_layer_pattern\n",
    "\t\t'''\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinput_size = self.m + self.n\n",
    "\t\t\n",
    "\t\t# add the first layer\n",
    "\t\tmodel.add(Dense(input_size, activation='relu', input_shape=(input_size,)))\n",
    "\n",
    "\t\t#one of the two model architectures tested\n",
    "\t\tif (hidden_layer_pattern == 'linear'):\n",
    "\t\t\tlinear_decrease = int(input_size/self.num_layers)\n",
    "\t\t\tfor i in range(self.num_layers):\n",
    "\t\t\t\tinput_size = input_size - linear_decrease\n",
    "\t\t\t\tmodel.add(Dense(input_size, activation='relu') )\n",
    "\n",
    "\t\tif (hidden_layer_pattern == 'exponential'):\n",
    "\t\t\texponential_decrease = int((np.exp(np.log(input_size)/(self.num_layers+2))))\n",
    "\t\t\tprint(exponential_decrease)\n",
    "\t\t\tfor i in range(self.num_layers):\n",
    "\t\t\t\tinput_size = int(input_size/exponential_decrease);\n",
    "\t\t\t\tmodel.add(Dense(input_size, activation='relu') )\n",
    "\n",
    "\t\tprint (model.output_shape)\n",
    "\t\t#one hot encoded output\n",
    "\t\tmodel.add(Dense(11, activation='relu'))\n",
    "\n",
    "\n",
    "\t\t# model says they optimized the log loss error\n",
    "\n",
    "\t\tadam = optimizers.Adam(lr=self.learn_rate, decay=.001)\n",
    "\t\tmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\t\tself.model = model\n",
    "\n",
    "\tdef train_model(self, model_number = 0):\n",
    "\t\t'''\n",
    "\t\tTrains the model. Saves checkpoints of the model at every epoch.\n",
    "\t\tI personally just stop training when I find that the loss function has barely changed. Since it takes\n",
    "\t\tso long to perform each epoch on my computer, I just keep running a 20 epoch train, stop it when I\n",
    "\t\thave to, then train again later.\n",
    "\t\tParam:\n",
    "\t\t\tmodel_number - Just changes the filename that the model is saved to. \n",
    "\t\t\t\t\t\t   Don't want to overwrite good save files during training, do you?\n",
    "\n",
    "\t\tNote: these checkpoints are 1GB each.\n",
    "\t\t'''\n",
    "\t\t# lets make checkpoints\n",
    "\t\tfilepath = \"nn_model_{}_lr_{}\".format(model_number,self.learn_rate)\n",
    "\t\tfilepath+= \"_{epoch:02d}.hdf5\"\n",
    "\n",
    "\t\tprint('learn_rate = {}'.format(self.learn_rate))\n",
    "\t\tcheckpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\t\tcallbacks_list = [checkpoint]\n",
    "\n",
    "\t\tself.model.fit(self.train_x, self.train_y, batch_size=128, epochs=20, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\tdef load_model(self, filename):\n",
    "\t\t'''\n",
    "\t\tLoads the weights of an identically architectured neural net at the given filepath\n",
    "\t\t'''\n",
    "\t\tself.model.load_weights(filename)\n",
    "\t\tadam = optimizers.Adam(lr=self.learn_rate, decay=.001)\n",
    "\t\tself.model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\tdef predict_values(self, test_type='validation'):\n",
    "\t\t'''\n",
    "\t\tPredicts values based on training or validation data\n",
    "\t\tReturn:\n",
    "\t\t\tscores\n",
    "\t\t\tpredicted values\n",
    "\t\t'''\n",
    "\t\t# print(self.model.get_weights())\n",
    "\t\tif (test_type == 'validation'):\n",
    "\t\t\tscores = self.model.predict(self.test_x, verbose=True)\n",
    "\t\t\treturn scores, self.test_y\n",
    "\t\telif (test_type == 'training'):\n",
    "\t\t\tscores = self.model.predict(self.train_x, verbose=True)\n",
    "\t\t\treturn scores, self.train_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
