{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation \n",
    "\n",
    "\n",
    "## The MovieLens Dataset\n",
    "Contains 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define file directories\n",
    "MOVIELENS_DIR = 'dat'\n",
    "USER_DATA_FILE = 'users.dat'\n",
    "MOVIE_DATA_FILE = 'movies.dat'\n",
    "RATING_DATA_FILE = 'ratings.dat'\n",
    "\n",
    "# Specify User's Age and Occupation Column\n",
    "AGES = { 1: \"Under 18\", 18: \"18-24\", 25: \"25-34\", 35: \"35-44\", 45: \"45-49\", 50: \"50-55\", 56: \"56+\" }\n",
    "OCCUPATIONS = { 0: \"other or not specified\", 1: \"academic/educator\", 2: \"artist\", 3: \"clerical/admin\",\n",
    "                4: \"college/grad student\", 5: \"customer service\", 6: \"doctor/health care\",\n",
    "                7: \"executive/managerial\", 8: \"farmer\", 9: \"homemaker\", 10: \"K-12 student\", 11: \"lawyer\",\n",
    "                12: \"programmer\", 13: \"retired\", 14: \"sales/marketing\", 15: \"scientist\", 16: \"self-employed\",\n",
    "                17: \"technician/engineer\", 18: \"tradesman/craftsman\", 19: \"unemployed\", 20: \"writer\" }\n",
    "\n",
    "# Define csv files to be saved into\n",
    "USERS_CSV_FILE = 'users.csv'\n",
    "MOVIES_CSV_FILE = 'movies.csv'\n",
    "RATINGS_CSV_FILE = 'ratings.csv'\n",
    "\n",
    "# Read the Ratings File\n",
    "ratings = pd.read_csv(os.path.join(MOVIELENS_DIR, RATING_DATA_FILE), \n",
    "                    sep='::', \n",
    "                    engine='python', \n",
    "                    encoding='latin-1',\n",
    "                    names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
    "\n",
    "# Set max_userid to the maximum user_id in the ratings\n",
    "max_userid = ratings['user_id'].drop_duplicates().max()\n",
    "# Set max_movieid to the maximum movie_id in the ratings\n",
    "max_movieid = ratings['movie_id'].drop_duplicates().max()\n",
    "\n",
    "# Process ratings dataframe for Keras Deep Learning model\n",
    "# Add user_emb_id column whose values == user_id - 1\n",
    "ratings['user_emb_id'] = ratings['user_id'] - 1\n",
    "# Add movie_emb_id column whose values == movie_id - 1\n",
    "ratings['movie_emb_id'] = ratings['movie_id'] - 1\n",
    "\n",
    "\n",
    "# Save into ratings.csv\n",
    "ratings.to_csv(RATINGS_CSV_FILE, \n",
    "               sep='\\t', \n",
    "               header=True, \n",
    "               encoding='latin-1', \n",
    "               columns=['user_id', 'movie_id', 'rating', 'timestamp', 'user_emb_id', 'movie_emb_id'])\n",
    "\n",
    "# Read the Users File\n",
    "users = pd.read_csv(os.path.join(MOVIELENS_DIR, USER_DATA_FILE), \n",
    "                    sep='::', \n",
    "                    engine='python', \n",
    "                    encoding='latin-1',\n",
    "                    names=['user_id', 'gender', 'age', 'occupation', 'zipcode'])\n",
    "users['age_desc'] = users['age'].apply(lambda x: AGES[x])\n",
    "users['occ_desc'] = users['occupation'].apply(lambda x: OCCUPATIONS[x])\n",
    "\n",
    "# Save into users.csv\n",
    "users.to_csv(USERS_CSV_FILE, \n",
    "             sep='\\t', \n",
    "             header=True, \n",
    "             encoding='latin-1',\n",
    "             columns=['user_id', 'gender', 'age', 'occupation', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Read the Movies File\n",
    "movies = pd.read_csv(os.path.join(MOVIELENS_DIR, MOVIE_DATA_FILE), \n",
    "                    sep='::', \n",
    "                    engine='python', \n",
    "                    encoding='latin-1',\n",
    "                    names=['movie_id', 'title', 'genres'])\n",
    "\n",
    "# Save into movies.csv\n",
    "movies.to_csv(MOVIES_CSV_FILE, \n",
    "              sep='\\t', \n",
    "              header=True, \n",
    "              columns=['movie_id', 'title', 'genres'])\n",
    "\n",
    "# Reading ratings file\n",
    "# Ignore the timestamp column\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "\n",
    "#sns.set_style('whitegrid')\n",
    "#sns.set(font_scale=1.5)\n",
    "#%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading ratings file\n",
    "# Ignore the timestamp column\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "# Join all 3 files into one dataframe\n",
    "dataset = pd.merge(pd.merge(movies, ratings),users)\n",
    "\n",
    "# Display 20 movies with highest ratings\n",
    "dataset[['title','genres','rating']].sort_values('rating', ascending=False).head(20)\n",
    "\n",
    "# Make a census of the genre keywords\n",
    "genre_labels = set()\n",
    "for s in movies['genres'].str.split('|').values:\n",
    "    genre_labels = genre_labels.union(set(s))\n",
    "\n",
    "# Function that counts the number of times each of the genre keywords appear\n",
    "def count_word(dataset, ref_col, census):\n",
    "    keyword_count = dict()\n",
    "    for s in census: \n",
    "        keyword_count[s] = 0\n",
    "    for census_keywords in dataset[ref_col].str.split('|'):        \n",
    "        if type(census_keywords) == float and pd.isnull(census_keywords): \n",
    "            continue        \n",
    "        for s in [s for s in census_keywords if s in census]: \n",
    "            if pd.notnull(s): \n",
    "                keyword_count[s] += 1\n",
    "    #______________________________________________________________________\n",
    "    # convert the dictionary in a list to sort the keywords by frequency\n",
    "    keyword_occurences = []\n",
    "    for k,v in keyword_count.items():\n",
    "        keyword_occurences.append([k,v])\n",
    "    keyword_occurences.sort(key = lambda x:x[1], reverse = True)\n",
    "    return keyword_occurences, keyword_count\n",
    "\n",
    "# Calling this function gives access to a list of genre keywords which are sorted by decreasing frequency\n",
    "keyword_occurences, dum = count_word(movies, 'genres', genre_labels)\n",
    "keyword_occurences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based filtering\n",
    "A Content-Based Recommendation Engine that computes similarity between movies based on movie genres. It will suggest movies that are most similar to a particular movie based on its genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Break up the big genre string into a string array\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "# Convert genres to string value\n",
    "movies['genres'] = movies['genres'].fillna(\"\").astype('str')\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(movies['genres'])\n",
    "tfidf_matrix.shape\n",
    "\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "cosine_sim[:4, :4]\n",
    "\n",
    "# Build a 1-dimensional array with movie titles\n",
    "titles = movies['title']\n",
    "indices = pd.Series(movies.index, index=movies['title'])\n",
    "\n",
    "# Function that get movie recommendations based on the cosine similarity score of movie genres\n",
    "def genre_recommendations(title):\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:21]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return titles.iloc[movie_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Fill NaN values in user_id and movie_id column with 0\n",
    "ratings['user_id'] = ratings['user_id'].fillna(0)\n",
    "ratings['movie_id'] = ratings['movie_id'].fillna(0)\n",
    "\n",
    "# Replace NaN values in rating column with average of all values\n",
    "ratings['rating'] = ratings['rating'].fillna(ratings['rating'].mean())\n",
    "\n",
    "# Randomly sample 1% of the ratings dataset\n",
    "small_data = ratings.sample(frac=0.02)\n",
    "\n",
    "train_data, test_data = cv.train_test_split(small_data, test_size=0.2)\n",
    "\n",
    "# Create two user-item matrices, one for training and another for testing\n",
    "train_data_matrix = train_data.as_matrix(columns = ['user_id', 'movie_id', 'rating'])\n",
    "test_data_matrix = test_data.as_matrix(columns = ['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "# Check their shape\n",
    "#print(train_data_matrix.shape)\n",
    "#print(test_data_matrix.shape)\n",
    "\n",
    "# User Similarity Matrix\n",
    "user_correlation = 1 - pairwise_distances(train_data, metric='correlation')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "#print(user_correlation[:4, :4])\n",
    "\n",
    "# Item Similarity Matrix\n",
    "item_correlation = 1 - pairwise_distances(train_data_matrix.T, metric='correlation')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "#print(item_correlation[:4, :4])\n",
    "\n",
    "# Function to predict ratings\n",
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        # Use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 1418.7354031\n",
      "Item-based CF RMSE: 1633.88175312\n",
      "User-based CF RMSE: 690.91724938\n",
      "Item-based CF RMSE: 228.717799499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(pred, actual))\n",
    "\n",
    "# Predict ratings on the training data with both similarity score\n",
    "user_prediction = predict(train_data_matrix, user_correlation, type='user')\n",
    "item_prediction = predict(train_data_matrix, item_correlation, type='item')\n",
    "\n",
    "# RMSE on the test data\n",
    "print('User-based CF RMSE on the test data:' + str(rmse(user_prediction, test_data_matrix)))\n",
    "print('Item-based CF RMSE on the test data:' + str(rmse(item_prediction, test_data_matrix)))\n",
    "\n",
    "# RMSE on the train data\n",
    "print('User-based CF RMSE on the train data:' + str(rmse(user_prediction, train_data_matrix)))\n",
    "print('Item-based CF RMSE on the train data:' + str(rmse(item_prediction, train_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models\n",
    "\n",
    "The basic idea is that the actual ratings of movies for each user can be represented by a matrix, say of users on the rows and movies along the columns.  \n",
    "We don’t have the full rating matrix, instead, we have a very sparse set of entries. \n",
    "But if we could factor the rating matrix into two separate matrices, say one that was Used by Latent Factors, and one that was Latent Factors by Movies, then we could find the user’s rating for any movie by taking the dot product of the User row and the Movie column.\n",
    "\n",
    "Y = Ratings.\n",
    "X1, X2 = Movies, Users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Embedding, Reshape, Merge\n",
    "from keras.models import Sequential\n",
    "\n",
    "class CFModel(Sequential):\n",
    "\n",
    "    # The constructor for the class\n",
    "    def __init__(self, n_users, m_items, k_factors, **kwargs):\n",
    "        # P is the embedding layer that creates an User by latent factors matrix.\n",
    "        # If the intput is a user_id, P returns the latent factor vector for that user.\n",
    "        P = Sequential()\n",
    "        P.add(Embedding(n_users, k_factors, input_length=1))\n",
    "        P.add(Reshape((k_factors,)))\n",
    "\n",
    "        # Q is the embedding layer that creates a Movie by latent factors matrix.\n",
    "        # If the input is a movie_id, Q returns the latent factor vector for that movie.\n",
    "        Q = Sequential()\n",
    "        Q.add(Embedding(m_items, k_factors, input_length=1))\n",
    "        Q.add(Reshape((k_factors,)))\n",
    "\n",
    "        super(CFModel, self).__init__(**kwargs)\n",
    "        \n",
    "        # The Merge layer takes the dot product of user and movie latent factor vectors to return the corresponding rating.\n",
    "        self.add(Merge([P, Q], mode='dot', dot_axes=1))\n",
    "\n",
    "    # The rate function to predict user's rating of unrated items\n",
    "    def rate(self, user_id, item_id):\n",
    "        return self.predict([np.array([user_id]), np.array([item_id])])[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print('Users:', Users, ', shape =', Users.shape)? (<ipython-input-1-09d23212270d>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-09d23212270d>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    print 'Users:', Users, ', shape =', Users.shape\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print('Users:', Users, ', shape =', Users.shape)?\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'user_emb_id', 'movie_emb_id', 'rating'])\n",
    "max_userid = ratings['user_id'].drop_duplicates().max()\n",
    "max_movieid = ratings['movie_id'].drop_duplicates().max()\n",
    "\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])\n",
    "\n",
    "# Create training set\n",
    "shuffled_ratings = ratings.sample(frac=1.)\n",
    "\n",
    "# Shuffling users\n",
    "Users = shuffled_ratings['user_emb_id'].values\n",
    "print 'Users:', Users, ', shape =', Users.shape\n",
    "\n",
    "# Shuffling movies\n",
    "Movies = shuffled_ratings['movie_emb_id'].values\n",
    "print 'Movies:', Movies, ', shape =', Movies.shape\n",
    "\n",
    "# Shuffling ratings\n",
    "Ratings = shuffled_ratings['rating'].values\n",
    "print 'Ratings:', Ratings, ', shape =', Ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CFModel.py:28: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  self.add(Merge([P, Q], mode='dot', dot_axes=1))\n",
      "/home/deb/anaconda2/lib/python2.7/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900188 samples, validate on 100021 samples\n",
      "Epoch 1/30\n",
      " - 408s - loss: 8.1697 - val_loss: 2.2583\n",
      "Epoch 2/30\n",
      " - 412s - loss: 1.4808 - val_loss: 1.1373\n",
      "Epoch 3/30\n",
      " - 408s - loss: 1.0038 - val_loss: 0.9509\n",
      "Epoch 4/30\n",
      " - 396s - loss: 0.8945 - val_loss: 0.8856\n",
      "Epoch 5/30\n",
      " - 417s - loss: 0.8465 - val_loss: 0.8504\n",
      "Epoch 6/30\n",
      " - 424s - loss: 0.8148 - val_loss: 0.8259\n",
      "Epoch 7/30\n",
      " - 415s - loss: 0.7901 - val_loss: 0.8098\n",
      "Epoch 8/30\n",
      " - 416s - loss: 0.7694 - val_loss: 0.7970\n",
      "Epoch 9/30\n",
      " - 419s - loss: 0.7493 - val_loss: 0.7870\n",
      "Epoch 10/30\n",
      " - 422s - loss: 0.7297 - val_loss: 0.7760\n",
      "Epoch 11/30\n",
      " - 416s - loss: 0.7099 - val_loss: 0.7685\n",
      "Epoch 12/30\n",
      " - 432s - loss: 0.6895 - val_loss: 0.7616\n",
      "Epoch 13/30\n",
      " - 433s - loss: 0.6689 - val_loss: 0.7563\n",
      "Epoch 14/30\n",
      " - 432s - loss: 0.6483 - val_loss: 0.7498\n",
      "Epoch 15/30\n",
      " - 412s - loss: 0.6273 - val_loss: 0.7484\n",
      "Epoch 16/30\n",
      " - 405s - loss: 0.6062 - val_loss: 0.7468\n",
      "Epoch 17/30\n",
      " - 407s - loss: 0.5855 - val_loss: 0.7443\n",
      "Epoch 18/30\n",
      " - 415s - loss: 0.5645 - val_loss: 0.7485\n",
      "Epoch 19/30\n",
      " - 419s - loss: 0.5443 - val_loss: 0.7500\n",
      "Minimum RMSE at epoch 17 = 0.8627\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2905</td>\n",
       "      <td>5.104347</td>\n",
       "      <td>Sanjuro (1962)</td>\n",
       "      <td>Action|Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3092</td>\n",
       "      <td>5.015607</td>\n",
       "      <td>Chushingura (1962)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>669</td>\n",
       "      <td>4.999459</td>\n",
       "      <td>Aparajito (1956)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1423</td>\n",
       "      <td>4.953498</td>\n",
       "      <td>Hearts and Minds (1996)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3338</td>\n",
       "      <td>4.925403</td>\n",
       "      <td>For All Mankind (1989)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>668</td>\n",
       "      <td>4.920885</td>\n",
       "      <td>Pather Panchali (1955)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1111</td>\n",
       "      <td>4.878110</td>\n",
       "      <td>Microcosmos (Microcosmos: Le peuple de l'herbe...</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1949</td>\n",
       "      <td>4.873702</td>\n",
       "      <td>Man for All Seasons, A (1966)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>670</td>\n",
       "      <td>4.868108</td>\n",
       "      <td>World of Apu, The (Apur Sansar) (1959)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>745</td>\n",
       "      <td>4.862697</td>\n",
       "      <td>Close Shave, A (1995)</td>\n",
       "      <td>Animation|Comedy|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3134</td>\n",
       "      <td>4.859750</td>\n",
       "      <td>Grand Illusion (Grande illusion, La) (1937)</td>\n",
       "      <td>Drama|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3022</td>\n",
       "      <td>4.857942</td>\n",
       "      <td>General, The (1927)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>649</td>\n",
       "      <td>4.833371</td>\n",
       "      <td>Cold Fever (Á köldum klaka) (1994)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>116</td>\n",
       "      <td>4.832465</td>\n",
       "      <td>Anne Frank Remembered (1995)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>916</td>\n",
       "      <td>4.818557</td>\n",
       "      <td>Roman Holiday (1953)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019</td>\n",
       "      <td>4.807254</td>\n",
       "      <td>Seven Samurai (The Magnificent Seven) (Shichin...</td>\n",
       "      <td>Action|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2920</td>\n",
       "      <td>4.802969</td>\n",
       "      <td>Children of Paradise (Les enfants du paradis) ...</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>326</td>\n",
       "      <td>4.796769</td>\n",
       "      <td>To Live (Huozhe) (1994)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>954</td>\n",
       "      <td>4.791075</td>\n",
       "      <td>Mr. Smith Goes to Washington (1939)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>922</td>\n",
       "      <td>4.788300</td>\n",
       "      <td>Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)</td>\n",
       "      <td>Film-Noir</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    movie_id  prediction                                              title  \\\n",
       "0       2905    5.104347                                     Sanjuro (1962)   \n",
       "1       3092    5.015607                                 Chushingura (1962)   \n",
       "2        669    4.999459                                   Aparajito (1956)   \n",
       "3       1423    4.953498                            Hearts and Minds (1996)   \n",
       "4       3338    4.925403                             For All Mankind (1989)   \n",
       "5        668    4.920885                             Pather Panchali (1955)   \n",
       "6       1111    4.878110  Microcosmos (Microcosmos: Le peuple de l'herbe...   \n",
       "7       1949    4.873702                      Man for All Seasons, A (1966)   \n",
       "8        670    4.868108             World of Apu, The (Apur Sansar) (1959)   \n",
       "9        745    4.862697                              Close Shave, A (1995)   \n",
       "10      3134    4.859750        Grand Illusion (Grande illusion, La) (1937)   \n",
       "11      3022    4.857942                                General, The (1927)   \n",
       "12       649    4.833371                 Cold Fever (Á köldum klaka) (1994)   \n",
       "13       116    4.832465                       Anne Frank Remembered (1995)   \n",
       "14       916    4.818557                               Roman Holiday (1953)   \n",
       "15      2019    4.807254  Seven Samurai (The Magnificent Seven) (Shichin...   \n",
       "16      2920    4.802969  Children of Paradise (Les enfants du paradis) ...   \n",
       "17       326    4.796769                            To Live (Huozhe) (1994)   \n",
       "18       954    4.791075                Mr. Smith Goes to Washington (1939)   \n",
       "19       922    4.788300      Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)   \n",
       "\n",
       "                       genres  \n",
       "0            Action|Adventure  \n",
       "1                       Drama  \n",
       "2                       Drama  \n",
       "3                       Drama  \n",
       "4                 Documentary  \n",
       "5                       Drama  \n",
       "6                 Documentary  \n",
       "7                       Drama  \n",
       "8                       Drama  \n",
       "9   Animation|Comedy|Thriller  \n",
       "10                  Drama|War  \n",
       "11                     Comedy  \n",
       "12               Comedy|Drama  \n",
       "13                Documentary  \n",
       "14             Comedy|Romance  \n",
       "15               Action|Drama  \n",
       "16              Drama|Romance  \n",
       "17                      Drama  \n",
       "18                      Drama  \n",
       "19                  Film-Noir  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define constants\n",
    "K_FACTORS = 100 # The number of dimensional embeddings for movies and users\n",
    "TEST_USER = 2000 # A random test user (user_id = 2000)\n",
    "\n",
    "# Define model\n",
    "model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "# Compile the model using MSE as the loss function and the AdaMax learning algorithm\n",
    "model.compile(loss='mse', optimizer='adamax')\n",
    "\n",
    "# Callbacks monitor the validation loss\n",
    "# Save the model weights each time the validation loss has improved\n",
    "callbacks = [EarlyStopping('val_loss', patience=2), \n",
    "             ModelCheckpoint('weights.h5', save_best_only=True)]\n",
    "\n",
    "# Use 30 epochs, 90% training data, 10% validation data \n",
    "history = model.fit([Users, Movies], Ratings, nb_epoch=30, validation_split=.1, verbose=2, callbacks=callbacks)\n",
    "\n",
    "# Show the best validation RMSE\n",
    "min_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history.history['val_loss']))\n",
    "print 'Minimum RMSE at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(math.sqrt(min_val_loss))\n",
    "\n",
    "# Use the pre-trained model\n",
    "trained_model = CFModel(max_userid, max_movieid, K_FACTORS)\n",
    "# Load weights\n",
    "trained_model.load_weights('weights.h5')\n",
    "\n",
    "# Pick a random test user\n",
    "users[users['user_id'] == TEST_USER]\n",
    "\n",
    "# Function to predict the ratings given User ID and Movie ID\n",
    "def predict_rating(user_id, movie_id):\n",
    "    return trained_model.rate(user_id - 1, movie_id - 1)\n",
    "\n",
    "user_ratings = ratings[ratings['user_id'] == TEST_USER][['user_id', 'movie_id', 'rating']]\n",
    "user_ratings['prediction'] = user_ratings.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
    "user_ratings.sort_values(by='rating', \n",
    "                         ascending=False).merge(movies, \n",
    "                                                on='movie_id', \n",
    "                                                how='inner', \n",
    "                                                suffixes=['_u', '_m']).head(20)\n",
    "\n",
    "recommendations = ratings[ratings['movie_id'].isin(user_ratings['movie_id']) == False][['movie_id']].drop_duplicates()\n",
    "recommendations['prediction'] = recommendations.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
    "recommendations.sort_values(by='prediction',\n",
    "                          ascending=False).merge(movies,\n",
    "                                                 on='movie_id',\n",
    "                                                 how='inner',\n",
    "                                                 suffixes=['_u', '_m']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class Collaborative_Filtering_Neural_Net(object):\n",
    "\n",
    "\tdef __init__(self, train_data, val_data, mask, num_layers=3, learn_rate=.2):\n",
    "\n",
    "\t\tself.train_data = train_data\n",
    "\t\tself.val_data   = val_data\n",
    "\t\tself.mask       = mask\n",
    "\t\tself.num_layers = num_layers\n",
    "\n",
    "\t\tself.m          = self.train_data.shape[0]\n",
    "\t\tself.n \t\t\t= self.train_data.shape[1]\n",
    "\t\t\n",
    "\t\tself.learn_rate = learn_rate\n",
    "\n",
    "\t\tself.construct_input()\n",
    "\n",
    "\n",
    "\tdef construct_input(self):\n",
    "\t\t'''\n",
    "\t\tConstruct training input/output from the training data matrix\n",
    "\t\tand \n",
    "\t\tConstruct validation input/output from the training/validation \n",
    "\t\t'''\n",
    "\t\tdef change_to_one_hot(value, value_range):\n",
    "\t\t\tone_hot_vec = np.zeros(len(value_range))\n",
    "\t\t\tone_hot_vec[int(value/.5)] = 1\n",
    "\t\t\treturn one_hot_vec\n",
    "\n",
    "\n",
    "\t\tm = self.m\n",
    "\t\tn = self.n\n",
    "\n",
    "\t\tuser_indices, movie_indices = (np.where(self.train_data > 0))\n",
    "\t\tscores = self.train_data[self.mask]\n",
    "\n",
    "\t\tnum_train_samples = user_indices.shape[0]\n",
    "\n",
    "\t\tself.train_x = np.zeros((num_train_samples, m+n))\n",
    "\t\tself.train_y = np.zeros((num_train_samples, 11))\n",
    "\n",
    "\t\tstart = time.time()\n",
    "\n",
    "\t\t#construct training input and output X, y\n",
    "\t\tfor i in range(num_train_samples):\n",
    "\t\t\tu_ind = user_indices[i]\n",
    "\t\t\tm_ind = movie_indices[i]\n",
    "\n",
    "\t\t\tself.train_x[i, u_ind]   = 1\n",
    "\t\t\tself.train_x[i, m+m_ind] = 1\n",
    "\n",
    "\t\t\tscore \t\t\t= self.train_data[u_ind, m_ind]\n",
    "\t\t\tself.train_y[i] = change_to_one_hot(score, np.arange(0,5.5,.5))\n",
    "\n",
    "\n",
    "\n",
    "\t\t#construct test inputs for where we need to predict values\n",
    "\t\tuser_indices, movie_indices = np.where(self.mask)\n",
    "\t\tnum_test_samples = user_indices.shape[0]\n",
    "\t\tself.test_x = np.zeros((num_test_samples, m+n))\n",
    "\t\tself.test_y = np.zeros((num_test_samples, 11))\n",
    "\n",
    "\t\tfor i in range(num_test_samples):\n",
    "\t\t\tu_ind = user_indices[i]\n",
    "\t\t\tm_ind = movie_indices[i]\n",
    "\n",
    "\t\t\tself.test_x[i, u_ind]   = 1\n",
    "\t\t\tself.test_x[i, m+m_ind] = 1\n",
    "\n",
    "\t\t\tscore \t\t   = self.val_data[u_ind, m_ind]\n",
    "\t\t\tself.test_y[i] = change_to_one_hot(score, np.arange(0,5.5,.5))\n",
    "\n",
    "\t\tprint(time.time() - start)\n",
    "\n",
    "\n",
    "\tdef construct_model(self, hidden_layer_pattern = 'exponential'):\n",
    "\t\t'''\n",
    "\t\tConstructs a Neural network with a given pattern.\n",
    "\t\tThe pattern indicates how many neurons should exist at every layer.\n",
    "\t\tParam:\n",
    "\t\t\thidden_layer_pattern - The input layer and output layer are fixed, but the rate at which the layer sizes\n",
    "\t\t\tdecreases depends on the parameter, hidden_layer_pattern\n",
    "\t\t'''\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinput_size = self.m + self.n\n",
    "\t\t\n",
    "\t\t# add the first layer\n",
    "\t\tmodel.add(Dense(input_size, activation='relu', input_shape=(input_size,)))\n",
    "\n",
    "\t\t#one of the two model architectures tested\n",
    "\t\tif (hidden_layer_pattern == 'linear'):\n",
    "\t\t\tlinear_decrease = int(input_size/self.num_layers)\n",
    "\t\t\tfor i in range(self.num_layers):\n",
    "\t\t\t\tinput_size = input_size - linear_decrease\n",
    "\t\t\t\tmodel.add(Dense(input_size, activation='relu') )\n",
    "\n",
    "\t\tif (hidden_layer_pattern == 'exponential'):\n",
    "\t\t\texponential_decrease = int((np.exp(np.log(input_size)/(self.num_layers+2))))\n",
    "\t\t\tprint(exponential_decrease)\n",
    "\t\t\tfor i in range(self.num_layers):\n",
    "\t\t\t\tinput_size = int(input_size/exponential_decrease);\n",
    "\t\t\t\tmodel.add(Dense(input_size, activation='relu') )\n",
    "\n",
    "\t\tprint (model.output_shape)\n",
    "\t\t#one hot encoded output\n",
    "\t\tmodel.add(Dense(11, activation='relu'))\n",
    "\n",
    "\n",
    "\t\t# model says they optimized the log loss error\n",
    "\n",
    "\t\tadam = optimizers.Adam(lr=self.learn_rate, decay=.001)\n",
    "\t\tmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\t\tself.model = model\n",
    "\n",
    "\tdef train_model(self, model_number = 0):\n",
    "\t\t'''\n",
    "\t\tTrains the model. Saves checkpoints of the model at every epoch.\n",
    "\t\tI personally just stop training when I find that the loss function has barely changed. Since it takes\n",
    "\t\tso long to perform each epoch on my computer, I just keep running a 20 epoch train, stop it when I\n",
    "\t\thave to, then train again later.\n",
    "\t\tParam:\n",
    "\t\t\tmodel_number - Just changes the filename that the model is saved to. \n",
    "\t\t\t\t\t\t   Don't want to overwrite good save files during training, do you?\n",
    "\n",
    "\t\tNote: these checkpoints are 1GB each.\n",
    "\t\t'''\n",
    "\t\t# lets make checkpoints\n",
    "\t\tfilepath = \"nn_model_{}_lr_{}\".format(model_number,self.learn_rate)\n",
    "\t\tfilepath+= \"_{epoch:02d}.hdf5\"\n",
    "\n",
    "\t\tprint('learn_rate = {}'.format(self.learn_rate))\n",
    "\t\tcheckpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\t\tcallbacks_list = [checkpoint]\n",
    "\n",
    "\t\tself.model.fit(self.train_x, self.train_y, batch_size=128, epochs=20, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "\tdef load_model(self, filename):\n",
    "\t\t'''\n",
    "\t\tLoads the weights of an identically architectured neural net at the given filepath\n",
    "\t\t'''\n",
    "\t\tself.model.load_weights(filename)\n",
    "\t\tadam = optimizers.Adam(lr=self.learn_rate, decay=.001)\n",
    "\t\tself.model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\tdef predict_values(self, test_type='validation'):\n",
    "\t\t'''\n",
    "\t\tPredicts values based on training or validation data\n",
    "\t\tReturn:\n",
    "\t\t\tscores\n",
    "\t\t\tpredicted values\n",
    "\t\t'''\n",
    "\t\t# print(self.model.get_weights())\n",
    "\t\tif (test_type == 'validation'):\n",
    "\t\t\tscores = self.model.predict(self.test_x, verbose=True)\n",
    "\t\t\treturn scores, self.test_y\n",
    "\t\telif (test_type == 'training'):\n",
    "\t\t\tscores = self.model.predict(self.train_x, verbose=True)\n",
    "\t\t\treturn scores, self.train_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import util\n",
    "from nn import Collaborative_Filtering_Neural_Net\n",
    "\n",
    "def train():\n",
    "\t'''\n",
    "\ttrains a neural net and saves snapshots every epoch. Runs 20 epochs or until you quit the process.\n",
    "\t'''\n",
    "\ttrain_mat, val_mat, masks = util.k_cross()\n",
    "\tA = util.load_data_matrix()\n",
    "\n",
    "\tstart = time.time()\n",
    "\tnet = Collaborative_Filtering_Neural_Net(train_mat[0], val_mat[0], masks[0])\n",
    "\tnet.learn_rate=.1\n",
    "\tnet.construct_model(hidden_layer_pattern = 'exponential')\n",
    "\t# net.load_model('nn_model_exponential_one_hot_learn_rate_.1_lr_0.1_04.hdf5')\n",
    "\tnet.train_model(model_number='exponential_one_hot')\n",
    "\tprint('time taken to train in seconds:', time.time() - start)\n",
    "\n",
    "def test(model_name = '', test_type = 'validation'):\n",
    "\t'''\n",
    "\tGets the accuracy and validation error of a model.\n",
    "\tThis function assumes you have been saving your models\n",
    "\t'''\n",
    "\ttrain_mat, val_mat, masks = util.k_cross()\n",
    "\tA = util.load_data_matrix()\n",
    "\n",
    "\tnet = Collaborative_Filtering_Neural_Net(train_mat[0], val_mat[0], masks[0])\n",
    "\tnet.learn_rate=.1\n",
    "\tnet.construct_model(hidden_layer_pattern = 'exponential')\n",
    "\tnet.load_model(model_name)\n",
    "\n",
    "\tpred_scores , true_scores= net.predict_values(test_type = test_type)\n",
    "\tpred_scores = pred_scores.argmax(axis=1)\n",
    "\ttrue_scores    = true_scores.argmax(axis=1)\n",
    "\n",
    "\t#get Accuracy\n",
    "\tnum_correct = np.sum(pred_scores == true_scores)\n",
    "\taccuracy    = num_correct/pred_scores.shape[0]*100\n",
    "\n",
    "\t#get MSE\n",
    "\terror = pred_scores-true_scores\n",
    "\tmse   = np.mean(np.power(error, 2))\n",
    "\n",
    "\tprint('The {} accuracy of the model is {}%'.format(test_type, accuracy))\n",
    "\tprint('The {} mean squared error of the model is {}'.format(test_type, mse))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttrain()\n",
    "\ttest('nn_model_exponential_one_hot_round_2_lr_0.1_08.hdf5', test_type='training')\n",
    "\ttest('nn_model_exponential_one_hot_round_2_lr_0.1_08.hdf5', test_type='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
